#!/usr/bin/env python3
"""
å®éªŒä¸€ï¼šç»Ÿä¸€åŸºçº¿æ¨¡å‹è®­ç»ƒè„šæœ¬
ç¡®ä¿ä¸ModernBERTä½¿ç”¨ç›¸åŒçš„è®­ç»ƒç­–ç•¥ï¼Œä¿è¯å®éªŒå…¬å¹³æ€§

åŠŸèƒ½:
1. ç»Ÿä¸€çš„è¶…å‚æ•°æœç´¢ç­–ç•¥
2. ç›¸åŒçš„äº¤å‰éªŒè¯æ–¹æ³•
3. ä¸€è‡´çš„è¯„ä¼°æŒ‡æ ‡
4. å…¬å¹³çš„å¯¹æ¯”åŸºç¡€

è¿è¡Œæ–¹å¼:
    python scripts/02_train_baselines_unified.py
"""

import os
import sys
import yaml
import json
import pickle
import logging
import argparse
import pandas as pd
import numpy as np
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–æ˜¾å­˜ç®¡ç†ï¼ˆæ–°ç‰ˆå˜é‡åï¼‰
# å‚è€ƒ PyTorch è­¦å‘Šï¼šPYTORCH_CUDA_ALLOC_CONF å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ PYTORCH_ALLOC_CONF
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"  # ç”¨äºè°ƒè¯•CUDAé”™è¯¯
from datetime import datetime
import torch
from sklearn.model_selection import StratifiedKFold, ParameterGrid
from sklearn.metrics import accuracy_score
import time
import hashlib

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))
experiment_root = Path(__file__).parent.parent
sys.path.insert(0, str(experiment_root))

# å¯¼å…¥æœ¬åœ°æ¨¡å‹ç®¡ç†å™¨
try:
    import importlib.util
    script_path = experiment_root / 'scripts' / '00_download_models.py'
    spec = importlib.util.spec_from_file_location("download_models", script_path)
    download_models = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(download_models)
    LocalModelManager = download_models.LocalModelManager
except Exception as e:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå®šä¹‰ä¸€ä¸ªç®€å•çš„å ä½ç¬¦ç±»
    print(f"è­¦å‘Šï¼šæ— æ³•å¯¼å…¥æœ¬åœ°æ¨¡å‹ç®¡ç†å™¨: {e}")
    class LocalModelManager:
        def __init__(self, experiment_root):
            self.experiment_root = experiment_root
        def download_all_required_models(self, force_redownload=False):
            return {}
        def is_model_downloaded(self, model_name):
            return False
        def list_downloaded_models(self):
            return {}

from src.models.base_classifiers import (
    RandomClassifier,
    RuleBasedClassifier,
    BertClassifier,
    RobertaClassifier
)
from src.utils.metrics import ClassificationMetrics, CalibrationMetrics
from src.utils.visualization import PerformancePlotter

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('outputs/logs/baseline_training_unified.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class UnifiedTrainingFramework:
    """ç»Ÿä¸€è®­ç»ƒæ¡†æ¶ - ç¡®ä¿æ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒç­–ç•¥ï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­å’Œæœ¬åœ°æ¨¡å‹ç®¡ç†"""
    
    def __init__(self, config, output_dir):
        self.config = config
        self.random_seed = config.get('random_seeds', {}).get('global_seed', 42)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.output_dir = Path(output_dir)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.model_dir = self.output_dir / 'models'
        self.results_dir = self.output_dir / 'results'
        self.checkpoint_dir = self.output_dir / 'checkpoints'
        
        for dir_path in [self.model_dir, self.results_dir, self.checkpoint_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥ç‚¹æ–‡ä»¶
        self.checkpoint_file = self.checkpoint_dir / 'training_progress.json'
        self.training_progress = self.load_training_progress()
        
        # æœ¬åœ°æ¨¡å‹ç®¡ç†å™¨
        self.local_model_manager = LocalModelManager(experiment_root)
        
        # æœ¬åœ°æ¨¡å‹é…ç½®
        self.use_local_models = config.get('local_models', {}).get('enabled', True)
        self.fallback_to_online = config.get('local_models', {}).get('fallback_to_online', True)
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(self.random_seed)
        torch.manual_seed(self.random_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(self.random_seed)
    
    def load_training_progress(self):
        """åŠ è½½è®­ç»ƒè¿›åº¦"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                logger.info(f"åŠ è½½è®­ç»ƒè¿›åº¦: {len(progress.get('completed_models', []))} ä¸ªæ¨¡å‹å·²å®Œæˆ")
                return progress
            except Exception as e:
                logger.warning(f"åŠ è½½è®­ç»ƒè¿›åº¦å¤±è´¥: {e}ï¼Œå°†é‡æ–°å¼€å§‹")
        
        return {
            'completed_models': [],
            'failed_models': [],
            'training_start_time': None,
            'last_update_time': None
        }
    
    def save_training_progress(self):
        """ä¿å­˜è®­ç»ƒè¿›åº¦"""
        self.training_progress['last_update_time'] = datetime.now().isoformat()
        try:
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(self.training_progress, f, indent=2, ensure_ascii=False)
            logger.debug("è®­ç»ƒè¿›åº¦å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜è®­ç»ƒè¿›åº¦å¤±è´¥: {e}")
    
    def is_model_completed(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å®Œæˆè®­ç»ƒ"""
        return model_name in self.training_progress['completed_models']
    
    def mark_model_completed(self, model_name: str):
        """æ ‡è®°æ¨¡å‹è®­ç»ƒå®Œæˆ"""
        if model_name not in self.training_progress['completed_models']:
            self.training_progress['completed_models'].append(model_name)
        self.save_training_progress()
    
    def mark_model_failed(self, model_name: str, error_msg: str):
        """æ ‡è®°æ¨¡å‹è®­ç»ƒå¤±è´¥"""
        failure_info = {
            'model_name': model_name,
            'error': str(error_msg),
            'timestamp': datetime.now().isoformat()
        }
        self.training_progress['failed_models'].append(failure_info)
        self.save_training_progress()
    
    def load_existing_model_result(self, model_name: str):
        """åŠ è½½å·²å­˜åœ¨çš„æ¨¡å‹ç»“æœ"""
        model_file = self.model_dir / f"{model_name}_unified.pkl"
        results_file = self.results_dir / f"{model_name}_unified_results.json"
        
        if not (model_file.exists() and results_file.exists()):
            return None
        
        try:
            # åŠ è½½æ¨¡å‹
            with open(model_file, 'rb') as f:
                model = pickle.load(f)
            
            # åŠ è½½ç»“æœ
            with open(results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            logger.info(f"[åŠ è½½] å·²å®Œæˆçš„æ¨¡å‹: {model_name}")
            
            # é‡æ„ç»“æœæ ¼å¼ä»¥åŒ¹é…è®­ç»ƒå‡½æ•°çš„è¿”å›å€¼
            return {
                'model': model,
                'evaluation': results['evaluation'],
                'best_params': results.get('best_params', {}),
                'cv_results': results.get('cv_results', {}),
                'predictions': None,  # è¿™äº›ä¼šåœ¨éœ€è¦æ—¶é‡æ–°è®¡ç®—
                'probabilities': None,
                'logits': None
            }
            
        except Exception as e:
            logger.warning(f"åŠ è½½å·²å­˜åœ¨æ¨¡å‹å¤±è´¥ {model_name}: {e}")
            return None
    
    def save_single_model_result(self, model_name: str, result: dict):
        """ç«‹å³ä¿å­˜å•ä¸ªæ¨¡å‹çš„ç»“æœ"""
        try:
            # ä¿å­˜æ¨¡å‹
            if result['model'] is not None:
                model_file = self.model_dir / f"{model_name}_unified.pkl"
                with open(model_file, 'wb') as f:
                    pickle.dump(result['model'], f)
                logger.info(f"[ä¿å­˜] æ¨¡å‹: {model_file}")
            
            # ä¿å­˜ç»“æœ
            results_file = self.results_dir / f"{model_name}_unified_results.json"
            
            serializable_result = {
                'model_name': model_name,
                'evaluation': result['evaluation'],
                'best_params': result.get('best_params', {}),
                'cv_results': result.get('cv_results', {}),
                'training_metadata': {
                    'framework': 'unified',
                    'timestamp': datetime.now().isoformat(),
                    'device': self.device,
                    'random_seed': self.random_seed
                }
            }
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_result, f, indent=2, ensure_ascii=False, default=str)
            logger.info(f"[ä¿å­˜] ç»“æœ: {results_file}")
            
            # æ ‡è®°å®Œæˆ
            self.mark_model_completed(model_name)
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹ç»“æœå¤±è´¥ {model_name}: {e}")
            self.mark_model_failed(model_name, str(e))
            return False
    
    def load_processed_data(self, data_dir: Path) -> tuple:
        """åŠ è½½å¤„ç†åçš„æ•°æ® - æ”¯æŒJSONæ ¼å¼"""
        logger.info("åŠ è½½å¤„ç†åçš„æ•°æ®...")
        
        # åŠ è½½JSONæ ¼å¼æ•°æ®
        with open(data_dir / 'train_data.json', 'r', encoding='utf-8') as f:
            train_data = json.load(f)
        with open(data_dir / 'calibration_data.json', 'r', encoding='utf-8') as f:
            calibration_data = json.load(f)
        with open(data_dir / 'test_data.json', 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X_train = [item['query'] for item in train_data]
        y_train = [{'zero_hop': 0, 'one_hop': 1, 'multi_hop': 2}[item['complexity']] for item in train_data]
        
        # å‡†å¤‡æ ¡å‡†æ•°æ®
        X_cal = [item['query'] for item in calibration_data]
        y_cal = [{'zero_hop': 0, 'one_hop': 1, 'multi_hop': 2}[item['complexity']] for item in calibration_data]
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        X_test = [item['query'] for item in test_data]
        y_test = [{'zero_hop': 0, 'one_hop': 1, 'multi_hop': 2}[item['complexity']] for item in test_data]
        
        logger.info(f"è®­ç»ƒé›†: {len(X_train)} æ¡")
        logger.info(f"æ ¡å‡†é›†: {len(X_cal)} æ¡")
        logger.info(f"æµ‹è¯•é›†: {len(X_test)} æ¡")
        
        return X_train, y_train, X_cal, y_cal, X_test, y_test
    
    def ensure_models_available(self, force_download: bool = False) -> bool:
        """ç¡®ä¿è®­ç»ƒæ‰€éœ€çš„æ¨¡å‹å·²ä¸‹è½½åˆ°æœ¬åœ°"""
        if not self.use_local_models:
            logger.info("ğŸŒ æœ¬åœ°æ¨¡å‹åŠŸèƒ½å·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨åœ¨çº¿æ¨¡å‹")
            return True
        
        logger.info("ğŸ” æ£€æŸ¥æœ¬åœ°æ¨¡å‹å¯ç”¨æ€§...")
        
        # éœ€è¦çš„æ¨¡å‹åˆ—è¡¨
        required_models = ['bert-base-cased', 'roberta-large']
        missing_models = []
        
        # æ£€æŸ¥æ¯ä¸ªæ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
        for model_name in required_models:
            if not self.local_model_manager.is_model_downloaded(model_name):
                missing_models.append(model_name)
                logger.warning(f"âŒ æ¨¡å‹æœªä¸‹è½½: {model_name}")
            else:
                logger.info(f"âœ… æ¨¡å‹å·²ä¸‹è½½: {model_name}")
        
        # å¦‚æœæœ‰ç¼ºå¤±çš„æ¨¡å‹ï¼Œå°è¯•ä¸‹è½½
        if missing_models or force_download:
            if missing_models:
                logger.info(f"ğŸ“¥ éœ€è¦ä¸‹è½½ {len(missing_models)} ä¸ªæ¨¡å‹: {missing_models}")
            
            if force_download:
                logger.info("ğŸ”„ å¼ºåˆ¶é‡æ–°ä¸‹è½½æ‰€æœ‰æ¨¡å‹...")
            
            # ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹
            download_results = self.local_model_manager.download_all_required_models(
                force_redownload=force_download
            )
            
            # æ£€æŸ¥ä¸‹è½½ç»“æœ
            all_downloaded = all(download_results.values())
            
            if all_downloaded:
                logger.info("âœ… æ‰€æœ‰æ¨¡å‹å·²æˆåŠŸä¸‹è½½åˆ°æœ¬åœ°")
                return True
            else:
                failed_models = [model for model, success in download_results.items() if not success]
                if self.fallback_to_online:
                    logger.warning(f"âš ï¸ éƒ¨åˆ†æ¨¡å‹ä¸‹è½½å¤±è´¥ {failed_models}ï¼Œå°†åœ¨è®­ç»ƒæ—¶å›é€€åˆ°åœ¨çº¿æ¨¡å‹")
                    return True
                else:
                    logger.error(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥ä¸”ç¦ç”¨åœ¨çº¿å›é€€: {failed_models}")
                    return False
        else:
            logger.info("âœ… æ‰€æœ‰æ‰€éœ€æ¨¡å‹å·²åœ¨æœ¬åœ°å¯ç”¨")
            return True
    
    def create_hyperparameter_grid(self, model_type: str):
        """åˆ›å»ºè¶…å‚æ•°æœç´¢ç½‘æ ¼ - ä¸ModernBERTä¸€è‡´"""
        if model_type in ['random', 'rule_based']:
            # ç®€å•æ¨¡å‹æ— éœ€è¶…å‚æ•°æœç´¢
            return [{}]
        
        # Transformeræ¨¡å‹ä½¿ç”¨ä¸ModernBERTç›¸åŒçš„æœç´¢ç©ºé—´
        search_config = self.config['training']['hyperparameter_search']
        
        # åŸºçº¿æ¨¡å‹ä½¿ç”¨ç›¸å¯¹ä¿å®ˆçš„æœç´¢ç©ºé—´ï¼ˆé¿å…è¿‡åº¦ä¼˜åŒ–ï¼‰
        param_grid = {
            'learning_rate': search_config['learning_rates'],
            'batch_size': search_config.get('batch_sizes', [16]),
            'max_epochs': [3, 5],  # æ¯”ModernBERTå°‘ä¸€äº›epoch
            'weight_decay': search_config.get('weight_decay', [0.01])
        }
        
        grid = list(ParameterGrid(param_grid))
        logger.info(f"{model_type} è¶…å‚æ•°ç½‘æ ¼ï¼Œå…± {len(grid)} ç»„åˆ")
        
        return grid
    
    def cross_validate_model(self, model_class, X_train, y_train, params, n_folds=5):
        """æ”¹è¿›çš„äº¤å‰éªŒè¯ - æ”¯æŒè®­ç»ƒè¿‡ç¨‹ç›‘æ§å’Œæ—©åœ"""
        from sklearn.model_selection import train_test_split
        
        logger.info(f"è¿›è¡Œ{n_folds}æŠ˜åµŒå¥—äº¤å‰éªŒè¯...")
        
        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=self.random_seed)
        cv_scores = []
        cv_ece_scores = []
        fold_histories = []
        
        for fold, (train_idx, test_idx) in enumerate(skf.split(X_train, y_train)):
            logger.info(f"  æŠ˜ {fold + 1}/{n_folds}")
            
            # è·å–foldçš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
            X_fold_train = [X_train[i] for i in train_idx]
            y_fold_train = [y_train[i] for i in train_idx]
            X_fold_test = [X_train[i] for i in test_idx]
            y_fold_test = [y_train[i] for i in test_idx]
            
            # åœ¨foldè®­ç»ƒé›†å†…éƒ¨å†åˆ†å‡ºéªŒè¯é›†ï¼ˆç”¨äºè®­ç»ƒç›‘æ§ï¼‰
            X_train_inner, X_val_inner, y_train_inner, y_val_inner = train_test_split(
                X_fold_train, y_fold_train, 
                test_size=0.15,  # 15%ä½œä¸ºéªŒè¯é›†
                stratify=y_fold_train,
                random_state=self.random_seed + fold
            )
            
            logger.info(f"    è®­ç»ƒé›†: {len(X_train_inner)}, éªŒè¯é›†: {len(X_val_inner)}, æµ‹è¯•é›†: {len(X_fold_test)}")
            
            try:
                # è®­ç»ƒæ¨¡å‹
                model = model_class(**params.get('model_params', {}))
                
                if hasattr(model, 'fit_with_params'):
                    # ä¸ºæ”¯æŒè¶…å‚æ•°çš„æ¨¡å‹æ·»åŠ éªŒè¯æ•°æ®
                    params_with_val = params.copy()
                    params_with_val['validation_data'] = (X_val_inner, y_val_inner)
                    params_with_val['early_stopping'] = True
                    params_with_val['patience'] = 2  # æ—©åœè€å¿ƒå€¼
                    
                    # è®­ç»ƒæ¨¡å‹å¹¶è·å–å†å²
                    history = model.fit_with_params(X_train_inner, y_train_inner, params_with_val)
                    
                    # éªŒè¯è¿”å›çš„æ˜¯å¦ä¸ºæœ‰æ•ˆçš„historyå­—å…¸
                    if isinstance(history, dict) and all(key in history for key in ['train_loss', 'train_acc', 'val_acc']):
                        fold_histories.append(history)
                    else:
                        logger.warning(f"fit_with_paramsè¿”å›äº†æ— æ•ˆçš„historyå¯¹è±¡: {type(history)}")
                        fold_histories.append(None)
                else:
                    # ç®€å•æ¨¡å‹
                    model.fit(X_train_inner, y_train_inner)
                    fold_histories.append(None)
                
                # åœ¨foldæµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ˆè¿™æ˜¯çœŸæ­£çš„äº¤å‰éªŒè¯è¯„ä¼°ï¼‰
                y_pred = model.predict(X_fold_test)
                y_proba = model.predict_proba(X_fold_test)
                
                # è®¡ç®—å‡†ç¡®ç‡
                accuracy = accuracy_score(y_fold_test, y_pred)
                cv_scores.append(accuracy)
                
                # è®¡ç®—ECE
                cal_metrics = CalibrationMetrics()
                calibration_results = cal_metrics.compute_all_calibration_metrics(
                    y_fold_test, y_proba, return_reliability_data=False
                )
                ece = calibration_results['ECE']
                cv_ece_scores.append(ece)
                
                logger.info(f"    æœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.4f}, ECE: {ece:.4f}")
                
            except Exception as e:
                logger.error(f"    æŠ˜ {fold + 1} è®­ç»ƒå¤±è´¥: {e}")
                # ä¸è¦ä½¿ç”¨å‡æ•°æ®ï¼è®©å¤±è´¥çœŸæ­£å¤±è´¥
                raise RuntimeError(f"äº¤å‰éªŒè¯æŠ˜ {fold + 1} è®­ç»ƒå¤±è´¥: {e}") from e
            finally:
                # æ¸…ç†æ˜¾å­˜ä¸ä¸´æ—¶å¯¹è±¡ï¼Œé¿å…åç»­æŠ˜ OOM
                try:
                    import gc
                    del model
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.reset_peak_memory_stats()
                except Exception:
                    pass
        
        mean_accuracy = np.mean(cv_scores)
        mean_ece = np.mean(cv_ece_scores)
        composite_score = mean_accuracy - mean_ece  # ä¸ModernBERTä¸€è‡´çš„è¯„åˆ†æ ‡å‡†
        
        logger.info(f"  äº¤å‰éªŒè¯å®Œæˆ - å¹³å‡å‡†ç¡®ç‡: {mean_accuracy:.4f}, å¹³å‡ECE: {mean_ece:.4f}")
        
        # å¯è§†åŒ–è®­ç»ƒå†å²ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if any(h is not None for h in fold_histories):
            self._plot_training_curves(fold_histories)
        
        return {
            'cv_accuracy': mean_accuracy,
            'cv_ece': mean_ece,
            'composite_score': composite_score,
            'cv_scores': cv_scores,
            'cv_ece_scores': cv_ece_scores,
            'fold_histories': fold_histories  # æ–°å¢ï¼šæ¯ä¸ªfoldçš„è®­ç»ƒå†å²
        }
    
    def _plot_training_curves(self, fold_histories):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        try:
            import matplotlib.pyplot as plt
            import os
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs('outputs/training_curves', exist_ok=True)
            
            # è¿‡æ»¤æœ‰æ•ˆçš„history
            valid_histories = []
            for i, history in enumerate(fold_histories):
                if history is None:
                    continue
                if not isinstance(history, dict):
                    logger.warning(f"fold {i} historyä¸æ˜¯å­—å…¸ç±»å‹: {type(history)}")
                    continue
                if not all(key in history for key in ['train_loss', 'train_acc']):
                    logger.warning(f"fold {i} historyç¼ºå°‘å¿…è¦å­—æ®µ: {list(history.keys()) if isinstance(history, dict) else 'not a dict'}")
                    continue
                valid_histories.append((i, history))
            
            if not valid_histories:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒå†å²å¯ç»˜åˆ¶")
                return
            
            # ä¸ºæ¯ä¸ªfoldç»˜åˆ¶æ›²çº¿
            fig, axes = plt.subplots(2, 3, figsize=(15, 10))
            fig.suptitle('Training Curves Across Folds', fontsize=16)
            
            for plot_idx, (fold_idx, history) in enumerate(valid_histories):
                if plot_idx >= 6:  # æœ€å¤šæ˜¾ç¤º6ä¸ªfold
                    break
                    
                row = plot_idx // 3
                col = plot_idx % 3
                ax = axes[row, col]
                
                epochs = range(1, len(history['train_loss']) + 1)
                
                # ç»˜åˆ¶æŸå¤±æ›²çº¿
                ax2 = ax.twinx()
                line1 = ax.plot(epochs, history['train_loss'], 'b-', label='Train Loss')
                line2 = ax.plot(epochs, history['train_acc'], 'g-', label='Train Acc')
                if history['val_acc']:
                    line3 = ax.plot(epochs, history['val_acc'], 'r-', label='Val Acc')
                
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Loss / Accuracy')
                ax.set_title(f'Fold {fold_idx + 1}')
                ax.legend()
                ax.grid(True, alpha=0.3)
            
            # éšè—ç©ºçš„å­å›¾
            for i in range(len(fold_histories), 6):
                row = i // 3
                col = i % 3
                if row < 2:
                    axes[row, col].set_visible(False)
            
            plt.tight_layout()
            plt.savefig('outputs/training_curves/fold_training_curves.png', dpi=150, bbox_inches='tight')
            plt.close()
            
            logger.info("è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° outputs/training_curves/fold_training_curves.png")
            
        except ImportError:
            logger.warning("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡è®­ç»ƒæ›²çº¿ç»˜åˆ¶")
        except Exception as e:
            logger.warning(f"ç»˜åˆ¶è®­ç»ƒæ›²çº¿æ—¶å‡ºé”™: {e}")
    
    def train_random_classifier(self, X_train, y_train, X_cal, y_cal, X_test, y_test):
        """è®­ç»ƒéšæœºåˆ†ç±»å™¨"""
        model_name = 'random'
        logger.info("=== è®­ç»ƒéšæœºåˆ†ç±»å™¨ ===")
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè®­ç»ƒ
        if self.is_model_completed(model_name):
            logger.info("ğŸ”„ éšæœºåˆ†ç±»å™¨å·²è®­ç»ƒå®Œæˆï¼ŒåŠ è½½å·²æœ‰ç»“æœ...")
            return self.load_existing_model_result(model_name)
        
        try:
            # æ— éœ€è¶…å‚æ•°æœç´¢
            clf = RandomClassifier(
                num_classes=3,
                random_state=self.random_seed
            )
            clf.fit(X_train, y_train)
            
            # è¯„ä¼°
            y_pred = clf.predict(X_test)
            y_proba = clf.predict_proba(X_test)
            logits = clf.get_logits(X_test)
            
            result = {
                'model': clf,
                'predictions': y_pred,
                'probabilities': y_proba,
                'logits': logits,
                'best_params': {},
                'cv_results': None
            }
            
            return result
            
        except Exception as e:
            logger.error(f"éšæœºåˆ†ç±»å™¨è®­ç»ƒå¤±è´¥: {e}")
            self.mark_model_failed(model_name, str(e))
            raise
    
    def train_rule_based_classifier(self, X_train, y_train, X_cal, y_cal, X_test, y_test):
        """è®­ç»ƒè§„åˆ™åˆ†ç±»å™¨"""
        model_name = 'rule_based'
        logger.info("=== è®­ç»ƒè§„åˆ™åˆ†ç±»å™¨ ===")
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè®­ç»ƒ
        if self.is_model_completed(model_name):
            logger.info("ğŸ”„ è§„åˆ™åˆ†ç±»å™¨å·²è®­ç»ƒå®Œæˆï¼ŒåŠ è½½å·²æœ‰ç»“æœ...")
            return self.load_existing_model_result(model_name)
        
        try:
            rule_config = self.config['models']['baselines']['rule_based']
            clf = RuleBasedClassifier(rule_config)
            clf.fit(X_train, y_train)
            
            # è¯„ä¼°
            y_pred = clf.predict(X_test)
            y_proba = clf.predict_proba(X_test)
            logits = clf.get_logits(X_test)
            
            result = {
                'model': clf,
                'predictions': y_pred,
                'probabilities': y_proba,
                'logits': logits,
                'best_params': rule_config,
                'cv_results': None
            }
            
            return result
            
        except Exception as e:
            logger.error(f"è§„åˆ™åˆ†ç±»å™¨è®­ç»ƒå¤±è´¥: {e}")
            self.mark_model_failed(model_name, str(e))
            raise
    
    def train_transformer_model(self, model_class, model_name, X_train, y_train, X_cal, y_cal, X_test, y_test):
        """è®­ç»ƒTransformeræ¨¡å‹ - ä½¿ç”¨ä¸ModernBERTç›¸åŒçš„ç­–ç•¥"""
        # å°†æ¨¡å‹åç§°æ˜ å°„åˆ°æ ‡å‡†åŒ–çš„é”®
        model_key_map = {
            'BERT-Base': 'bert_base',
            'RoBERTa-Large': 'roberta_large',
            'ModernBERT': 'modernbert'
        }
        model_key = model_key_map.get(model_name, model_name.lower().replace('-', '_'))
        
        logger.info(f"=== è®­ç»ƒ {model_name} ===")
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè®­ç»ƒ
        if self.is_model_completed(model_key):
            logger.info(f"ğŸ”„ {model_name} å·²è®­ç»ƒå®Œæˆï¼ŒåŠ è½½å·²æœ‰ç»“æœ...")
            return self.load_existing_model_result(model_key)
        
        try:
            # è¶…å‚æ•°æœç´¢
            param_grid = self.create_hyperparameter_grid(model_name.lower())
            
            best_score = -float('inf')
            best_params = None
            best_model = None
            best_cv_results = None
            all_results = []
            
            for i, params in enumerate(param_grid):
                logger.info(f"\n--- {model_name} è¶…å‚æ•°ç»„åˆ {i+1}/{len(param_grid)} ---")
                logger.info(f"å‚æ•°: {params}")
                
                try:
                    # å‡†å¤‡æ¨¡å‹å‚æ•°
                    model_config = self.config['models']['baselines'][model_name.lower().replace('-', '_')]
                    model_params = {
                        'model_name': model_config['model_name'],
                        'num_classes': model_config['num_classes'],
                        'device': self.device,
                        'use_local_model': self.use_local_models,
                        'fallback_to_online': self.fallback_to_online
                    }
                    params['model_params'] = model_params
                    
                    # äº¤å‰éªŒè¯
                    cv_results = self.cross_validate_model(
                        model_class, X_train, y_train, params,
                        n_folds=self.config['training']['cross_validation']['n_folds']
                    )
                    
                    all_results.append({
                        'params': params,
                        'cv_results': cv_results
                    })
                    
                    # æ›´æ–°æœ€ä½³æ¨¡å‹
                    if cv_results['composite_score'] > best_score:
                        best_score = cv_results['composite_score']
                        best_params = params
                        best_cv_results = cv_results
                        logger.info(f"å‘ç°æ›´å¥½çš„å‚æ•°! ç»¼åˆåˆ†æ•°: {best_score:.4f}")
                    
                except Exception as e:
                    logger.error(f"å‚æ•°ç»„åˆè®­ç»ƒå¤±è´¥: {e}")
                    continue
        
            # ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒ
            if best_params is not None:
                logger.info(f"\nä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒ {model_name}...")
                logger.info(f"æœ€ä½³å‚æ•°: {best_params}")
                
                try:
                    model = model_class(**best_params['model_params'])
                    
                    if hasattr(model, 'fit_with_params'):
                        model.fit_with_params(X_train, y_train, best_params)
                    else:
                        model.fit(X_train, y_train)
                    
                    # æµ‹è¯•é›†è¯„ä¼°
                    y_pred = model.predict(X_test)
                    y_proba = model.predict_proba(X_test)
                    logits = model.get_logits(X_test)
                    
                    best_model = model
                    
                except Exception as e:
                    logger.error(f"æœ€ç»ˆè®­ç»ƒå¤±è´¥: {e}")
                    self.mark_model_failed(model_key, str(e))
                    raise
            else:
                error_msg = f"{model_name} è®­ç»ƒå®Œå…¨å¤±è´¥ï¼Œæ‰€æœ‰å‚æ•°ç»„åˆéƒ½å¤±è´¥"
                logger.error(error_msg)
                self.mark_model_failed(model_key, error_msg)
                raise RuntimeError(error_msg)
            
            result = {
                'model': best_model,
                'predictions': y_pred,
                'probabilities': y_proba,
                'logits': logits,
                'best_params': best_params,
                'cv_results': best_cv_results,
                'all_results': all_results
            }
            
            return result
            
        except Exception as e:
            logger.error(f"{model_name} è®­ç»ƒå¤±è´¥: {e}")
            self.mark_model_failed(model_key, str(e))
            raise
    
    def evaluate_model(self, y_true, y_pred, y_proba, model_name):
        """ç»Ÿä¸€çš„æ¨¡å‹è¯„ä¼° - ä¸ModernBERTå®Œå…¨ä¸€è‡´"""
        logger.info(f"è¯„ä¼° {model_name} æ€§èƒ½...")
        
        # åˆ†ç±»æŒ‡æ ‡
        clf_metrics = ClassificationMetrics()
        classification_results = clf_metrics.compute_all_metrics(y_true, y_pred, y_proba)
        
        # æ ¡å‡†æŒ‡æ ‡
        cal_metrics = CalibrationMetrics()
        calibration_results = cal_metrics.compute_all_calibration_metrics(
            y_true, y_proba, return_reliability_data=True
        )
        
        # åˆå¹¶ç»“æœ
        results = {
            'model_name': model_name,
            'classification': classification_results,
            'calibration': calibration_results,
            'timestamp': datetime.now().isoformat()
        }
        
        # è®°å½•å…³é”®æŒ‡æ ‡
        logger.info(f"{model_name} ç»“æœ:")
        logger.info(f"  å‡†ç¡®ç‡: {classification_results['accuracy']:.4f}")
        logger.info(f"  å®F1: {classification_results['macro_f1']:.4f}")
        logger.info(f"  ECE: {calibration_results['ECE']:.4f}")
        
        return results


def main():
    """ä¸»å‡½æ•° - æ”¯æŒæ–­ç‚¹ç»­è®­"""
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€åŸºçº¿æ¨¡å‹è®­ç»ƒ - æ”¯æŒæ–­ç‚¹ç»­è®­')
    parser.add_argument('--config', type=str, default='config/model_config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='outputs', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--resume', action='store_true', help='ä»æ–­ç‚¹ç»§ç»­è®­ç»ƒ')
    parser.add_argument('--force_retrain', type=str, nargs='+', help='å¼ºåˆ¶é‡æ–°è®­ç»ƒæŒ‡å®šæ¨¡å‹')
    parser.add_argument('--download_models', action='store_true', help='å¼ºåˆ¶é‡æ–°ä¸‹è½½æ‰€éœ€æ¨¡å‹')
    parser.add_argument('--use_online_models', action='store_true', help='ç¦ç”¨æœ¬åœ°æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨åœ¨çº¿æ¨¡å‹')
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config_path = Path(args.config)
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # å¤„ç†å‘½ä»¤è¡Œå‚æ•°å¯¹é…ç½®çš„è¦†ç›–
    if args.use_online_models:
        logger.info("ğŸŒ å‘½ä»¤è¡ŒæŒ‡å®šä½¿ç”¨åœ¨çº¿æ¨¡å‹ï¼Œç¦ç”¨æœ¬åœ°æ¨¡å‹åŠŸèƒ½")
        config.setdefault('local_models', {})['enabled'] = False
    
    # åˆå§‹åŒ–æ¡†æ¶ï¼ˆç°åœ¨éœ€è¦output_dirå‚æ•°ï¼‰
    framework = UnifiedTrainingFramework(config, args.output_dir)
    
    # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
    if framework.training_progress['training_start_time'] is None:
        framework.training_progress['training_start_time'] = datetime.now().isoformat()
        framework.save_training_progress()
    
    # ç¡®ä¿æ‰€éœ€æ¨¡å‹å·²ä¸‹è½½åˆ°æœ¬åœ°
    logger.info(f"\n{'='*70}")
    logger.info("[å‡†å¤‡] æ£€æŸ¥å’Œä¸‹è½½æ‰€éœ€æ¨¡å‹...")
    logger.info(f"{'='*70}")
    
    models_available = framework.ensure_models_available(force_download=args.download_models)
    if not models_available:
        logger.error("âŒ æ— æ³•ç¡®ä¿æ¨¡å‹å¯ç”¨æ€§ï¼Œé€€å‡ºè®­ç»ƒ")
        sys.exit(1)
    
    # åŠ è½½æ•°æ®
    data_dir = Path(config['data']['processed_data_dir'])
    X_train, y_train, X_cal, y_cal, X_test, y_test = framework.load_processed_data(data_dir)
    
    # å®šä¹‰æ‰€æœ‰è¦è®­ç»ƒçš„æ¨¡å‹
    models_to_train = [
        ('random', 'RandomClassifier', framework.train_random_classifier),
        ('rule_based', 'RuleBasedClassifier', framework.train_rule_based_classifier),
        ('bert_base', 'BertClassifier', lambda *args: framework.train_transformer_model(BertClassifier, 'BERT-Base', *args)),
        ('roberta_large', 'RobertaClassifier', lambda *args: framework.train_transformer_model(RobertaClassifier, 'RoBERTa-Large', *args))
    ]
    
    # å¤„ç†å¼ºåˆ¶é‡æ–°è®­ç»ƒ
    if args.force_retrain:
        for model_name in args.force_retrain:
            if model_name in framework.training_progress['completed_models']:
                framework.training_progress['completed_models'].remove(model_name)
                logger.info(f"ğŸ”„ å¼ºåˆ¶é‡æ–°è®­ç»ƒ: {model_name}")
        framework.save_training_progress()
    
    # è®­ç»ƒæ‰€æœ‰åŸºçº¿æ¨¡å‹
    model_results = {}
    total_models = len(models_to_train)
    completed_count = len(framework.training_progress['completed_models'])
    
    logger.info(f"\n[å¼€å§‹] è®­ç»ƒæµç¨‹ - æ€»å…± {total_models} ä¸ªæ¨¡å‹ï¼Œå·²å®Œæˆ {completed_count} ä¸ª")
    
    for model_key, model_display_name, train_func in models_to_train:
        logger.info(f"\n{'='*70}")
        logger.info(f"[å¤„ç†] æ¨¡å‹: {model_display_name} ({model_key})")
        logger.info(f"[è¿›åº¦] {len(model_results)+1}/{total_models}")
        
        try:
            start_time = time.time()
            
            # è®­ç»ƒæ¨¡å‹
            result = train_func(X_train, y_train, X_cal, y_cal, X_test, y_test)
            
            # å¦‚æœéœ€è¦é‡æ–°è®¡ç®—é¢„æµ‹ï¼ˆä»åŠ è½½çš„æ¨¡å‹ï¼‰
            if result['predictions'] is None and result['model'] is not None:
                logger.info("ğŸ”„ é‡æ–°è®¡ç®—é¢„æµ‹ç»“æœ...")
                result['predictions'] = result['model'].predict(X_test)
                result['probabilities'] = result['model'].predict_proba(X_test)
                result['logits'] = result['model'].get_logits(X_test)
            
            # è¯„ä¼°æ¨¡å‹
            evaluation = framework.evaluate_model(
                y_test, result['predictions'], result['probabilities'], model_display_name
            )
            
            # å®Œæ•´ç»“æœ
            complete_result = {
                **result,
                'evaluation': evaluation
            }
            
            # ç«‹å³ä¿å­˜ç»“æœ
            if framework.save_single_model_result(model_key, complete_result):
                model_results[model_key] = complete_result
                    
                training_time = time.time() - start_time
                logger.info(f"[å®Œæˆ] {model_display_name} è®­ç»ƒå®Œæˆ! ç”¨æ—¶: {training_time:.1f}ç§’")
                logger.info(f"[ç»“æœ] å‡†ç¡®ç‡: {evaluation['classification']['accuracy']:.4f}")
                logger.info(f"[ç»“æœ] ECE: {evaluation['calibration']['ECE']:.4f}")
            else:
                logger.error(f"âŒ {model_display_name} ä¿å­˜å¤±è´¥")
                
        except Exception as e:
            logger.error(f"âŒ {model_display_name} è®­ç»ƒå¤±è´¥: {e}")
            # ç»§ç»­è®­ç»ƒä¸‹ä¸€ä¸ªæ¨¡å‹ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
            continue
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    logger.info(f"\n{'='*70}")
    logger.info("[å®Œæˆ] åŸºçº¿æ¨¡å‹è®­ç»ƒæµç¨‹å®Œæˆ!")
    logger.info(f"{'='*70}")
    
    if model_results:
        summary_data = []
        for model_name, result in model_results.items():
            eval_result = result['evaluation']
            summary_data.append({
                'Model': eval_result['model_name'],
                'Accuracy': f"{eval_result['classification']['accuracy']:.4f}",
                'Macro-F1': f"{eval_result['classification']['macro_f1']:.4f}",
                'ECE': f"{eval_result['calibration']['ECE']:.4f}",
                'Composite Score': f"{eval_result['classification']['accuracy'] - eval_result['calibration']['ECE']:.4f}"
            })
        
        summary_df = pd.DataFrame(summary_data)
        print("\n[æ±‡æ€»] è®­ç»ƒç»“æœ:")
        print(summary_df.to_string(index=False))
        
        # ä¿å­˜æ±‡æ€»
        # ç»Ÿä¸€ç»“æœæ–‡ä»¶å‘½åï¼Œä¾›åç»­è¯„ä¼°è„šæœ¬ä½¿ç”¨
        summary_file = framework.results_dir / 'baseline_models_summary_unified.csv'
        summary_df.to_csv(summary_file, index=False)
        logger.info(f"ğŸ“ æ±‡æ€»ç»“æœä¿å­˜è‡³: {summary_file}")

        # é¢å¤–è¾“å‡ºå•æ¨¡å‹ç»“æœæ–‡ä»¶ï¼Œå‘½åä¸ 05_evaluation.py è¯»å–ä¿æŒä¸€è‡´
        try:
            for model_key, complete_result in model_results.items():
                mapped = {
                    'random': 'random',
                    'rule_based': 'rule_based',
                    'bert_base': 'bert',
                    'roberta_large': 'roberta'
                }.get(model_key, model_key)
                single_file = framework.results_dir / f"{mapped}_results.json"
                with open(single_file, 'w', encoding='utf-8') as f:
                    json.dump(complete_result['evaluation'], f, indent=2, ensure_ascii=False, default=str)
                logger.info(f"[ä¿å­˜] è¯„ä¼°æ‘˜è¦: {single_file}")
        except Exception as e:
            logger.warning(f"å¯¼å‡ºè¯„ä¼°æ‘˜è¦æ–‡ä»¶å¤±è´¥: {e}")
        
        # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡
        total_completed = len(framework.training_progress['completed_models'])
        total_failed = len(framework.training_progress['failed_models'])
        
        logger.info(f"\n[ç»Ÿè®¡] è®­ç»ƒç»Ÿè®¡:")
        logger.info(f"   [æˆåŠŸ] å®Œæˆ: {total_completed} ä¸ªæ¨¡å‹")
        logger.info(f"   âŒ è®­ç»ƒå¤±è´¥: {total_failed} ä¸ªæ¨¡å‹")
        logger.info(f"   ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {framework.model_dir}")
        logger.info(f"   [ä¿å­˜] ç»“æœä½ç½®: {framework.results_dir}")
        
        if total_failed > 0:
            logger.info(f"\nâš ï¸  å¤±è´¥çš„æ¨¡å‹:")
            for failure in framework.training_progress['failed_models']:
                logger.info(f"   â€¢ {failure['model_name']}: {failure['error']}")
    else:
        logger.warning("âš ï¸  æ²¡æœ‰æˆåŠŸå®Œæˆä»»ä½•æ¨¡å‹çš„è®­ç»ƒ")
    
        logger.info(f"\n[ä¿¡æ¯] æ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨äº†ç›¸åŒçš„è®­ç»ƒç­–ç•¥ï¼Œç¡®ä¿äº†å®éªŒçš„å…¬å¹³æ€§")
    logger.info(f"ğŸ”„ æ”¯æŒæ–­ç‚¹ç»­è®­ - ä½¿ç”¨ --resume å‚æ•°å¯ä»æ–­ç‚¹ç»§ç»­")
    logger.info(f"ğŸ”§ å¼ºåˆ¶é‡è®­ - ä½¿ç”¨ --force_retrain å‚æ•°å¯é‡æ–°è®­ç»ƒæŒ‡å®šæ¨¡å‹")
    logger.info(f"ğŸ“¥ æ¨¡å‹ä¸‹è½½ - ä½¿ç”¨ --download_models å‚æ•°å¯é‡æ–°ä¸‹è½½æ¨¡å‹")
    logger.info(f"ğŸŒ åœ¨çº¿æ¨¡å¼ - ä½¿ç”¨ --use_online_models å‚æ•°å¯ç›´æ¥ä½¿ç”¨åœ¨çº¿æ¨¡å‹")
    
    # æ˜¾ç¤ºæœ¬åœ°æ¨¡å‹çŠ¶æ€
    if framework.use_local_models:
        logger.info(f"\n[æœ¬åœ°æ¨¡å‹] çŠ¶æ€ä¿¡æ¯:")
        downloaded_models = framework.local_model_manager.list_downloaded_models()
        if downloaded_models:
            logger.info(f"   âœ… å·²ä¸‹è½½ {len(downloaded_models)} ä¸ªæ¨¡å‹:")
            for local_name, info in downloaded_models.items():
                model_path = Path(info['local_path'])
                logger.info(f"      â€¢ {info['hf_name']} â†’ {model_path.name}")
        else:
            logger.info(f"   âš ï¸ æ— æœ¬åœ°æ¨¡å‹ï¼Œè®­ç»ƒæ—¶å°†ä½¿ç”¨åœ¨çº¿æ¨¡å‹")
    else:
        logger.info(f"\n[åœ¨çº¿æ¨¡å¼] å°†ç›´æ¥ä» Hugging Face åŠ è½½æ¨¡å‹")


if __name__ == "__main__":
    main()