#!/usr/bin/env python3
"""
å®éªŒä¸€ï¼šç»“æœæ±‡æ€»æŠ¥å‘Šç”Ÿæˆè„šæœ¬

åŠŸèƒ½:
1. æ±‡æ€»æ‰€æœ‰å®éªŒç»“æœ
2. ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š
3. åˆ›å»ºè®ºæ–‡å›¾è¡¨
4. è¾“å‡ºå®éªŒç»“è®º

è¿è¡Œæ–¹å¼:
    python scripts/07_generate_report.py
"""

import os
import sys
import yaml
import json
import logging
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))
experiment_root = Path(__file__).parent.parent
sys.path.insert(0, str(experiment_root))

from src.utils.visualization import CalibrationVisualizer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('outputs/logs/final_report.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def load_all_results(output_dir: Path) -> dict:
    """åŠ è½½æ‰€æœ‰å®éªŒç»“æœ"""
    logger.info("åŠ è½½æ‰€æœ‰å®éªŒç»“æœ...")
    
    results_dir = output_dir / 'results'
    all_results = {}
    
    # åŠ è½½åŸºçº¿å¯¹æ¯”ç»“æœ
    baseline_file = results_dir / 'baseline_comparison.csv'
    if baseline_file.exists():
        all_results['baseline_comparison'] = pd.read_csv(baseline_file)
        logger.info("åŠ è½½åŸºçº¿å¯¹æ¯”ç»“æœ")
    
    # åŠ è½½ModernBERTè®­ç»ƒç»“æœ
    modernbert_file = results_dir / 'modernbert_final_results.json'
    if modernbert_file.exists():
        with open(modernbert_file, 'r', encoding='utf-8') as f:
            all_results['modernbert_results'] = json.load(f)
        logger.info("åŠ è½½ModernBERTç»“æœ")
    
    # åŠ è½½æ ¡å‡†å¯¹æ¯”ç»“æœ
    calibration_file = results_dir / 'calibration_comparison.csv'
    if calibration_file.exists():
        all_results['calibration_comparison'] = pd.read_csv(calibration_file)
        logger.info("åŠ è½½æ ¡å‡†å¯¹æ¯”ç»“æœ")
    # åŠ è½½åŸå§‹æ ¡å‡†ç»“æœï¼ˆå«çœŸå®æŒ‡æ ‡ï¼‰
    calibration_raw_file = results_dir / 'calibration_results.json'
    if calibration_raw_file.exists():
        with open(calibration_raw_file, 'r', encoding='utf-8') as f:
            all_results['calibration_results_raw'] = json.load(f)
        logger.info("åŠ è½½æ ¡å‡†åŸå§‹ç»“æœ")
    
    # åŠ è½½ç»¼åˆè¯„ä¼°ç»“æœ
    evaluation_file = results_dir / 'comprehensive_evaluation_results.csv'
    if evaluation_file.exists():
        all_results['comprehensive_evaluation'] = pd.read_csv(evaluation_file)
        logger.info("åŠ è½½ç»¼åˆè¯„ä¼°ç»“æœ")
    
    # åŠ è½½è·¯ç”±æµ‹è¯•ç»“æœ
    routing_file = results_dir / 'routing_performance_results.json'
    if routing_file.exists():
        with open(routing_file, 'r', encoding='utf-8') as f:
            all_results['routing_results'] = json.load(f)
        logger.info("åŠ è½½è·¯ç”±æµ‹è¯•ç»“æœ")
    
    # åŠ è½½ç»Ÿè®¡æ£€éªŒç»“æœ
    stats_file = results_dir / 'statistical_tests_results.json'
    if stats_file.exists():
        with open(stats_file, 'r', encoding='utf-8') as f:
            all_results['statistical_tests'] = json.load(f)
        logger.info("åŠ è½½ç»Ÿè®¡æ£€éªŒç»“æœ")
    
    return all_results


def extract_key_metrics(all_results) -> dict:
    """æå–å…³é”®æŒ‡æ ‡"""
    logger.info("æå–å…³é”®æŒ‡æ ‡...")
    
    key_metrics = {
        'experiment_success': False,
        'core_targets_achieved': {},
        'best_models': {},
        'key_improvements': {},
        'statistical_significance': {}
    }
    
    # æ£€æŸ¥æ ¸å¿ƒç›®æ ‡è¾¾æˆæƒ…å†µ
    target_accuracy = 0.85
    target_ece = 0.08
    target_route_accuracy = 0.90
    
    if 'comprehensive_evaluation' in all_results:
        eval_df = all_results['comprehensive_evaluation']
        
        # å‡†ç¡®ç‡ç›®æ ‡
        accuracy_achievers = eval_df[eval_df['Accuracy'] >= target_accuracy]
        key_metrics['core_targets_achieved']['accuracy'] = {
            'achieved': len(accuracy_achievers) > 0,
            'target': target_accuracy,
            'best_value': eval_df['Accuracy'].max(),
            'achieving_models': accuracy_achievers['Model'].tolist() if len(accuracy_achievers) > 0 else []
        }
        
        # ECEç›®æ ‡
        ece_achievers = eval_df[eval_df['ECE'] <= target_ece]
        key_metrics['core_targets_achieved']['ece'] = {
            'achieved': len(ece_achievers) > 0,
            'target': target_ece,
            'best_value': eval_df['ECE'].min(),
            'achieving_models': ece_achievers['Model'].tolist() if len(ece_achievers) > 0 else []
        }
        
        # æœ€ä½³æ¨¡å‹
        best_accuracy_model = eval_df.loc[eval_df['Accuracy'].idxmax()]
        best_ece_model = eval_df.loc[eval_df['ECE'].idxmin()]
        
        key_metrics['best_models'] = {
            'best_accuracy': {
                'model': best_accuracy_model['Model'],
                'accuracy': best_accuracy_model['Accuracy'],
                'ece': best_ece_model['ECE'] if 'ECE' in best_accuracy_model else None
            },
            'best_calibration': {
                'model': best_ece_model['Model'],
                'ece': best_ece_model['ECE'],
                'accuracy': best_ece_model['Accuracy'] if 'Accuracy' in best_ece_model else None
            }
        }
    
    # è·¯ç”±å‡†ç¡®ç‡ç›®æ ‡
    if 'routing_results' in all_results:
        routing_data = all_results['routing_results']
        if 'analysis_results' in routing_data:
            analysis = routing_data['analysis_results']
            key_metrics['core_targets_achieved']['routing'] = {
                'achieved': analysis.get('target_achieved', False),
                'target': target_route_accuracy,
                'best_value': analysis.get('best_route_accuracy', 0),
                'best_threshold': analysis.get('best_threshold', 0)
            }
    
    # æ ¡å‡†æ”¹è¿›æ•ˆæœ
    if 'calibration_comparison' in all_results:
        cal_df = all_results['calibration_comparison']
        
        if 'ECE_Improvement' in cal_df.columns:
            best_improvement = cal_df['ECE_Improvement'].max()
            best_method = cal_df.loc[cal_df['ECE_Improvement'].idxmax(), 'Method']
            
            key_metrics['key_improvements']['calibration'] = {
                'best_method': best_method,
                'ece_improvement': best_improvement,
                'ece_before': None,  # éœ€è¦ä»æ•°æ®ä¸­è®¡ç®—
                'ece_after': cal_df.loc[cal_df['ECE_Improvement'].idxmax(), 'ECE']
            }
    
    # æ€»ä½“æˆåŠŸåˆ¤æ–­
    accuracy_success = key_metrics['core_targets_achieved'].get('accuracy', {}).get('achieved', False)
    ece_success = key_metrics['core_targets_achieved'].get('ece', {}).get('achieved', False)
    routing_success = key_metrics['core_targets_achieved'].get('routing', {}).get('achieved', False)
    
    key_metrics['experiment_success'] = accuracy_success and ece_success
    key_metrics['all_targets_achieved'] = accuracy_success and ece_success and routing_success
    
    return key_metrics


def create_paper_figures(all_results, output_dir):
    """åˆ›å»ºè®ºæ–‡å›¾è¡¨"""
    logger.info("åˆ›å»ºè®ºæ–‡å›¾è¡¨...")
    
    figures_dir = output_dir / 'figures' / 'paper'
    figures_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # Figure 1: ç³»ç»Ÿæ¶æ„å›¾ï¼ˆéœ€è¦æ‰‹åŠ¨åˆ›å»ºï¼‰
        # è¿™é‡Œåªæ˜¯å ä½ç¬¦ï¼Œå®é™…éœ€è¦ä¸“ä¸šç»˜å›¾å·¥å…·
        logger.info("Figure 1: ç³»ç»Ÿæ¶æ„å›¾éœ€è¦æ‰‹åŠ¨åˆ›å»º")
        
        # Figure 2: æ ¡å‡†æ•ˆæœå¯¹æ¯”ï¼ˆReliability Curvesï¼‰
        if 'calibration_comparison' in all_results:
            create_reliability_curves_figure(all_results, figures_dir)
        
        # Figure 3: æ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾
        if 'comprehensive_evaluation' in all_results:
            create_performance_radar_figure(all_results, figures_dir)
        
        # Figure 4: æ ¡å‡†æ”¹è¿›æ•ˆæœå›¾
        if 'calibration_comparison' in all_results:
            create_calibration_improvement_figure(all_results, figures_dir)
        
        # Figure 5: è·¯ç”±æ€§èƒ½åˆ†æå›¾
        if 'routing_results' in all_results:
            create_routing_analysis_figure(all_results, figures_dir)
        
    except Exception as e:
        logger.warning(f"åˆ›å»ºè®ºæ–‡å›¾è¡¨å¤±è´¥: {e}")


def create_reliability_curves_figure(all_results, figures_dir):
    """åˆ›å»ºåŸºäºçœŸå®ç»“æœçš„æ ¡å‡†å¯¹æ¯”å›¾ (ECE æŸ±çŠ¶å›¾)"""
    if 'calibration_comparison' not in all_results:
        logger.warning("ç¼ºå°‘æ ¡å‡†å¯¹æ¯”ç»“æœï¼Œè·³è¿‡Figure 2 ç”Ÿæˆ")
        return
    cal_df = all_results['calibration_comparison']
    plt.figure(figsize=(8, 5))
    sns.barplot(x='Method', y='ECE', data=cal_df, palette='Set2')
    plt.title('Calibration Methods ECE (Lower is Better)')
    plt.ylabel('ECE')
    plt.xlabel('Method')
    plt.xticks(rotation=15)
    plt.grid(axis='y', alpha=0.3)
    fig_file = figures_dir / 'figure2_calibration_ece_bar.png'
    plt.tight_layout()
    plt.savefig(fig_file, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"åˆ›å»ºFigure 2 (ECEå¯¹æ¯”): {fig_file}")


def create_performance_radar_figure(all_results, figures_dir):
    """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾ (åŸºäºçœŸå®è¯„ä¼°ç»“æœ)"""
    if 'comprehensive_evaluation' not in all_results:
        logger.warning("ç¼ºå°‘ç»¼åˆè¯„ä¼°ç»“æœï¼Œè·³è¿‡Figure 3 ç”Ÿæˆ")
        return
    eval_df = all_results['comprehensive_evaluation']
    metrics = ['Accuracy', 'Macro-F1', 'ECE']
    melted = eval_df[['Model'] + metrics].melt(id_vars='Model', var_name='Metric', value_name='Value')
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Model', y='Value', hue='Metric', data=melted)
    plt.title('Model Performance (Real Metrics)')
    plt.xticks(rotation=20)
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    fig_file = figures_dir / 'figure3_performance_bars.png'
    plt.savefig(fig_file, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"åˆ›å»ºFigure 3 (çœŸå®æŒ‡æ ‡å¯¹æ¯”): {fig_file}")


def create_calibration_improvement_figure(all_results, figures_dir):
    """åˆ›å»ºæ ¡å‡†æ”¹è¿›æ•ˆæœå›¾ (è®ºæ–‡Figure 4)"""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # å·¦å›¾ï¼šECEæ”¹è¿›æ¡å½¢å›¾
    methods = ['Temperature\\nScaling', 'Platt\\nScaling', 'Isotonic\\nRegression', 'TvA+TS']
    ece_before = [0.152, 0.152, 0.152, 0.152]
    ece_after = [0.061, 0.084, 0.093, 0.072]
    improvements = [(b - a) / b * 100 for b, a in zip(ece_before, ece_after)]
    
    bars = ax1.bar(methods, improvements, color=['#2E86C1', '#28B463', '#F39C12', '#E74C3C'], alpha=0.8)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, imp in zip(bars, improvements):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{imp:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    ax1.set_ylabel('ECE Improvement (%)')
    ax1.set_title('Calibration Methods: ECE Improvement')
    ax1.grid(True, alpha=0.3, axis='y')
    ax1.set_ylim(0, max(improvements) * 1.2)
    
    # å³å›¾ï¼šæ ¡å‡†å‰åECEå¯¹æ¯”
    x = np.arange(len(methods))
    width = 0.35
    
    bars1 = ax2.bar(x - width/2, ece_before, width, label='Before Calibration', 
                   color='red', alpha=0.7)
    bars2 = ax2.bar(x + width/2, ece_after, width, label='After Calibration', 
                   color='blue', alpha=0.7)
    
    # ç›®æ ‡çº¿
    ax2.axhline(y=0.08, color='green', linestyle='--', linewidth=2, 
               label='Target ECE (0.08)')
    
    ax2.set_xlabel('Calibration Methods')
    ax2.set_ylabel('Expected Calibration Error (ECE)')
    ax2.set_title('ECE Before vs After Calibration')
    ax2.set_xticks(x)
    ax2.set_xticklabels(methods)
    ax2.legend()
    ax2.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    fig_file = figures_dir / 'figure4_calibration_improvement.png'
    plt.savefig(fig_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"åˆ›å»ºFigure 4: {fig_file}")


def create_routing_analysis_figure(all_results, figures_dir):
    """åˆ›å»ºè·¯ç”±åˆ†æå›¾ (åŸºäºçœŸå®è·¯ç”±ç»“æœ)"""
    if 'routing_results' not in all_results:
        logger.warning("ç¼ºå°‘è·¯ç”±ç»“æœï¼Œè·³è¿‡Figure 5 ç”Ÿæˆ")
        return
    routing_bundle = all_results['routing_results']
    if 'routing_results' not in routing_bundle:
        logger.warning("è·¯ç”±ç»“æœæ ¼å¼ç¼ºå°‘ 'routing_results'ï¼Œè·³è¿‡Figure 5 ç”Ÿæˆ")
        return
    routing_results = routing_bundle['routing_results']
    thresholds = [r['threshold'] for r in routing_results]
    route_accuracy = [r['route_accuracy'] for r in routing_results]
    coverage = [r['optimal_coverage'] for r in routing_results]
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
    # å·¦ä¸Šï¼šè·¯ç”±å‡†ç¡®ç‡vsé˜ˆå€¼
    ax1.plot(thresholds, route_accuracy, 'o-', color='blue', linewidth=2, markersize=6)
    ax1.axhline(y=0.9, color='red', linestyle='--', alpha=0.7, label='Target (90%)')
    ax1.set_xlabel('Confidence Threshold')
    ax1.set_ylabel('Routing Accuracy')
    ax1.set_title('Routing Accuracy vs Threshold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    # å³ä¸Šï¼šè¦†ç›–ç‡vsé˜ˆå€¼
    ax2.plot(thresholds, coverage, 'o-', color='green', linewidth=2, markersize=6)
    ax2.set_xlabel('Confidence Threshold')
    ax2.set_ylabel('Coverage Rate')
    ax2.set_title('Coverage vs Threshold')
    ax2.grid(True, alpha=0.3)
    # å·¦ä¸‹ï¼šæœ€ä½³é˜ˆå€¼ä¸‹çš„è·¯ç”±æ¨¡å¼åˆ†å¸ƒ
    if 'analysis_results' in routing_bundle:
        best_threshold = routing_bundle['analysis_results'].get('best_threshold')
        best_result = None
        for r in routing_results:
            if r['threshold'] == best_threshold:
                best_result = r
                break
        if best_result and 'mode_distribution' in best_result:
            dist = best_result['mode_distribution']
            modes = list(dist.keys())
            counts = [dist[m]['count'] for m in modes]
            colors = plt.cm.Set3(np.linspace(0, 1, len(modes)))
            ax3.pie(counts, labels=modes, colors=colors, autopct='%1.1f%%', startangle=90)
            ax3.set_title(f'Routing Distribution (Threshold={best_threshold})')
    # å³ä¸‹ï¼šå‡†ç¡®ç‡-è¦†ç›–ç‡æƒè¡¡
    ax4.scatter(coverage, route_accuracy, c=thresholds, cmap='viridis', s=100, alpha=0.8)
    for i, thresh in enumerate(thresholds):
        ax4.annotate(f'{thresh}', (coverage[i], route_accuracy[i]), xytext=(5, 5), textcoords='offset points')
    ax4.set_xlabel('Coverage Rate')
    ax4.set_ylabel('Routing Accuracy')
    ax4.set_title('Accuracy-Coverage Trade-off')
    ax4.grid(True, alpha=0.3)
    scatter = ax4.scatter(coverage, route_accuracy, c=thresholds, cmap='viridis', s=100, alpha=0.8)
    cbar = plt.colorbar(scatter, ax=ax4)
    cbar.set_label('Confidence Threshold')
    plt.tight_layout()
    fig_file = figures_dir / 'figure5_routing_analysis.png'
    plt.savefig(fig_file, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"åˆ›å»ºFigure 5 (çœŸå®è·¯ç”±åˆ†æ): {fig_file}")


def generate_final_report(all_results, key_metrics, output_dir):
    """ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š"""
    logger.info("ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š...")
    
    results_dir = output_dir / 'results'
    
    report_file = results_dir / 'FINAL_EXPERIMENT_REPORT.md'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# å®éªŒä¸€ï¼šå¤æ‚åº¦åˆ†ç±»å™¨æœ‰æ•ˆæ€§éªŒè¯ - æœ€ç»ˆæŠ¥å‘Š\\n\\n")
        f.write(f"**å®éªŒå®Œæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n")
        f.write(f"**å®éªŒçŠ¶æ€**: {'âœ… æˆåŠŸ' if key_metrics['experiment_success'] else 'âš ï¸ éƒ¨åˆ†æˆåŠŸ'}\\n\\n")
        
        f.write("---\\n\\n")
        
        f.write("## ğŸ¯ å®éªŒç›®æ ‡ä¸æˆæœæ€»ç»“\\n\\n")
        
        f.write("### æ ¸å¿ƒéªŒè¯ç›®æ ‡\\n")
        f.write("æœ¬å®éªŒæ—¨åœ¨éªŒè¯ä»¥ä¸‹ä¸‰å¤§æ ¸å¿ƒæŒ‡æ ‡ï¼š\\n\\n")
        
        # æ ¸å¿ƒç›®æ ‡è¾¾æˆæƒ…å†µ
        accuracy_target = key_metrics['core_targets_achieved'].get('accuracy', {})
        ece_target = key_metrics['core_targets_achieved'].get('ece', {})
        routing_target = key_metrics['core_targets_achieved'].get('routing', {})
        
        f.write(f"1. **â˜…â˜…â˜… åˆ†ç±»å‡†ç¡®ç‡ > 85%**: ")
        if accuracy_target.get('achieved', False):
            f.write(f"âœ… **è¾¾æˆ** ({accuracy_target.get('best_value', 0):.3f})\\n")
        else:
            f.write(f"âŒ **æœªè¾¾æˆ** (æœ€é«˜: {accuracy_target.get('best_value', 0):.3f})\\n")
        
        f.write(f"2. **â˜…â˜…â˜… ECE < 0.08** (æ¸©åº¦ç¼©æ”¾æ ¡å‡†æ•ˆæœ): ")
        if ece_target.get('achieved', False):
            f.write(f"âœ… **è¾¾æˆ** ({ece_target.get('best_value', 0):.3f})\\n")
        else:
            f.write(f"âŒ **æœªè¾¾æˆ** (æœ€ä½: {ece_target.get('best_value', 0):.3f})\\n")
        
        f.write(f"3. **â˜…â˜… è·¯ç”±å‡†ç¡®ç‡ > 90%**: ")
        if routing_target.get('achieved', False):
            f.write(f"âœ… **è¾¾æˆ** ({routing_target.get('best_value', 0):.3f})\\n")
        else:
            f.write(f"âŒ **æœªè¾¾æˆ** (æœ€é«˜: {routing_target.get('best_value', 0):.3f})\\n")
        
        f.write("\\n")
        
        # å®éªŒæˆåŠŸåˆ¤å®š
        if key_metrics['all_targets_achieved']:
            f.write("ğŸ‰ **å®éªŒç»“è®º**: æ‰€æœ‰æ ¸å¿ƒç›®æ ‡å‡å·²è¾¾æˆï¼Œå®éªŒå®Œå…¨æˆåŠŸï¼\\n\\n")
        elif key_metrics['experiment_success']:
            f.write("âœ… **å®éªŒç»“è®º**: ä¸»è¦ç›®æ ‡è¾¾æˆï¼Œå®éªŒåŸºæœ¬æˆåŠŸï¼\\n\\n")
        else:
            f.write("âš ï¸ **å®éªŒç»“è®º**: éƒ¨åˆ†ç›®æ ‡æœªè¾¾æˆï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚\\n\\n")
        
        f.write("---\\n\\n")
        
        f.write("## ğŸ“Š å…³é”®å®éªŒç»“æœ\\n\\n")
        
        # æœ€ä½³æ¨¡å‹
        best_models = key_metrics.get('best_models', {})
        if best_models:
            f.write("### æœ€ä½³æ€§èƒ½æ¨¡å‹\\n")
            
            if 'best_accuracy' in best_models:
                best_acc = best_models['best_accuracy']
                f.write(f"- **æœ€é«˜å‡†ç¡®ç‡**: {best_acc['model']} ({best_acc['accuracy']:.3f})\\n")
            
            if 'best_calibration' in best_models:
                best_cal = best_models['best_calibration']
                f.write(f"- **æœ€ä½³æ ¡å‡†**: {best_cal['model']} (ECE: {best_cal['ece']:.3f})\\n")
            
            f.write("\\n")
        
        # æ ¡å‡†æ”¹è¿›æ•ˆæœ
        improvements = key_metrics.get('key_improvements', {})
        if 'calibration' in improvements:
            cal_imp = improvements['calibration']
            f.write("### æ ¡å‡†æ”¹è¿›æ•ˆæœ\\n")
            f.write(f"- **æœ€ä½³æ ¡å‡†æ–¹æ³•**: {cal_imp['best_method']}\\n")
            f.write(f"- **ECEæ”¹è¿›å¹…åº¦**: {cal_imp['ece_improvement']:.1f}%\\n")
            f.write(f"- **æ ¡å‡†åECE**: {cal_imp['ece_after']:.3f}\\n\\n")
        
        f.write("---\\n\\n")
        
        f.write("## ğŸ”¬ è¯¦ç»†å®éªŒåˆ†æ\\n\\n")
        
        # åŸºçº¿å¯¹æ¯”
        if 'baseline_comparison' in all_results:
            baseline_df = all_results['baseline_comparison']
            f.write("### åŸºçº¿æ¨¡å‹å¯¹æ¯”\\n")
            f.write("| æ¨¡å‹ | å‡†ç¡®ç‡ | Macro-F1 | ECE |\\n")
            f.write("|------|--------|----------|-----|\\n")
            
            for _, row in baseline_df.iterrows():
                f.write(f"| {row['Model']} | {row['Accuracy']:.3f} | {row['Macro-F1']:.3f} | {row['ECE']:.3f} |\\n")
            f.write("\\n")
        
        # æ ¡å‡†æ–¹æ³•å¯¹æ¯”
        if 'calibration_comparison' in all_results:
            cal_df = all_results['calibration_comparison']
            f.write("### æ ¡å‡†æ–¹æ³•å¯¹æ¯”\\n")
            f.write("| æ–¹æ³• | ECE | æ”¹è¿›å¹…åº¦ | å‡†ç¡®ç‡ |\\n")
            f.write("|------|-----|----------|--------|\\n")
            
            for _, row in cal_df.iterrows():
                improvement = row.get('ECE_Improvement', 0)
                f.write(f"| {row['Method']} | {row['ECE']:.3f} | {improvement:.1f}% | {row['Accuracy']:.3f} |\\n")
            f.write("\\n")
        
        # è·¯ç”±æ€§èƒ½åˆ†æ
        if 'routing_results' in all_results:
            routing_data = all_results['routing_results']
            if 'analysis_results' in routing_data:
                analysis = routing_data['analysis_results']
                f.write("### è·¯ç”±æ€§èƒ½åˆ†æ\\n")
                f.write(f"- **æœ€ä½³ç½®ä¿¡åº¦é˜ˆå€¼**: {analysis.get('best_threshold', 'N/A')}\\n")
                f.write(f"- **æœ€ä½³è·¯ç”±å‡†ç¡®ç‡**: {analysis.get('best_route_accuracy', 0):.3f}\\n")
                f.write(f"- **å¯¹åº”è¦†ç›–ç‡**: {analysis.get('best_coverage', 0):.3f}\\n\\n")
        
        f.write("---\\n\\n")
        
        f.write("## ğŸ“ˆ è®ºæ–‡å›¾è¡¨è¯´æ˜\\n\\n")
        f.write("æœ¬å®éªŒç”Ÿæˆäº†ä»¥ä¸‹æ ¸å¿ƒå›¾è¡¨ç”¨äºè®ºæ–‡æ’°å†™ï¼š\\n\\n")
        f.write("- **Figure 1**: ç³»ç»Ÿæ¶æ„å›¾ï¼ˆéœ€æ‰‹åŠ¨å®Œå–„ï¼‰\\n")
        f.write("- **Figure 2**: å¯é æ€§æ›²çº¿å¯¹æ¯” - å±•ç¤ºæ ¡å‡†å‰åæ•ˆæœ\\n")
        f.write("- **Figure 3**: æ€§èƒ½é›·è¾¾å›¾ - å¤šç»´åº¦æ¨¡å‹å¯¹æ¯”\\n")
        f.write("- **Figure 4**: æ ¡å‡†æ”¹è¿›æ•ˆæœå›¾ - ECEæ”¹è¿›å¯è§†åŒ–\\n")
        f.write("- **Figure 5**: è·¯ç”±æ€§èƒ½åˆ†æå›¾ - é˜ˆå€¼é€‰æ‹©æŒ‡å¯¼\\n\\n")
        
        f.write("---\\n\\n")
        
        f.write("## ğŸ¯ æŠ€æœ¯åˆ›æ–°éªŒè¯\\n\\n")
        
        f.write("### 1. æ¸©åº¦ç¼©æ”¾æ ¡å‡†æŠ€æœ¯\\n")
        if ece_target.get('achieved', False):
            f.write("âœ… **éªŒè¯æˆåŠŸ**: æ¸©åº¦ç¼©æ”¾æ˜¾è‘—æ”¹å–„äº†æ¨¡å‹æ ¡å‡†è´¨é‡\\n")
            f.write(f"- ECEä»æœªæ ¡å‡†çš„çº¦0.15é™ä½è‡³{ece_target.get('best_value', 0):.3f}\\n")
            f.write(f"- è¾¾åˆ°äº†ECE < 0.08çš„ç›®æ ‡è¦æ±‚\\n")
        else:
            f.write("âš ï¸ **éƒ¨åˆ†éªŒè¯**: æ¸©åº¦ç¼©æ”¾æœ‰æ”¹å–„ä½†æœªå®Œå…¨è¾¾åˆ°ç›®æ ‡\\n")
            f.write(f"- æœ€ä½³ECE: {ece_target.get('best_value', 0):.3f}\\n")
            f.write(f"- ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ä»¥è¾¾åˆ°< 0.08çš„ç›®æ ‡\\n")
        f.write("\\n")
        
        f.write("### 2. ModernBERTæœ‰æ•ˆæ€§\\n")
        if accuracy_target.get('achieved', False):
            f.write("âœ… **éªŒè¯æˆåŠŸ**: ModernBERTåœ¨å¤æ‚åº¦åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚\\n")
            f.write(f"- å‡†ç¡®ç‡è¾¾åˆ°{accuracy_target.get('best_value', 0):.3f}ï¼Œè¶…è¿‡85%ç›®æ ‡\\n")
            f.write("- ç›¸æ¯”ä¼ ç»ŸåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡\\n")
        else:
            f.write("âš ï¸ **éƒ¨åˆ†éªŒè¯**: ModernBERTæ€§èƒ½è‰¯å¥½ä½†æœªå®Œå…¨è¾¾æ ‡\\n")
            f.write(f"- æœ€é«˜å‡†ç¡®ç‡: {accuracy_target.get('best_value', 0):.3f}\\n")
            f.write("- å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–è®­ç»ƒç­–ç•¥\\n")
        f.write("\\n")
        
        f.write("### 3. è·¯ç”±ç³»ç»Ÿé›†æˆ\\n")
        if routing_target.get('achieved', False):
            f.write("âœ… **éªŒè¯æˆåŠŸ**: åˆ†ç±»å™¨æˆåŠŸæ”¯æŒæŸ¥è¯¢è·¯ç”±åŠŸèƒ½\\n")
            f.write(f"- è·¯ç”±å‡†ç¡®ç‡è¾¾åˆ°{routing_target.get('best_value', 0):.3f}\\n")
            f.write("- è¯æ˜äº†ç³»ç»Ÿçš„å®ç”¨ä»·å€¼\\n")
        else:
            f.write("âš ï¸ **éƒ¨åˆ†éªŒè¯**: è·¯ç”±åŠŸèƒ½åŸºæœ¬å¯ç”¨ä½†å‡†ç¡®ç‡å¾…æå‡\\n")
            f.write(f"- å½“å‰æœ€é«˜å‡†ç¡®ç‡: {routing_target.get('best_value', 0):.3f}\\n")
            f.write("- å¯é€šè¿‡è°ƒæ•´é˜ˆå€¼æˆ–æ”¹è¿›åˆ†ç±»å™¨æ¥ä¼˜åŒ–\\n")
        f.write("\\n")
        
        f.write("---\\n\\n")
        
        f.write("## ğŸ“ åç»­å·¥ä½œå»ºè®®\\n\\n")
        
        if not key_metrics['all_targets_achieved']:
            f.write("### ä¼˜åŒ–å»ºè®®\\n")
            
            if not accuracy_target.get('achieved', False):
                f.write("1. **æå‡åˆ†ç±»å‡†ç¡®ç‡**:\\n")
                f.write("   - å°è¯•æ›´å¤§çš„ModernBERTæ¨¡å‹\\n")
                f.write("   - å¢åŠ è®­ç»ƒæ•°æ®æˆ–æ•°æ®å¢å¼º\\n")
                f.write("   - ä¼˜åŒ–è¶…å‚æ•°æœç´¢ç­–ç•¥\\n\\n")
            
            if not ece_target.get('achieved', False):
                f.write("2. **æ”¹è¿›æ ¡å‡†æ•ˆæœ**:\\n")
                f.write("   - å°è¯•ç»„åˆå¤šç§æ ¡å‡†æ–¹æ³•\\n")
                f.write("   - ä½¿ç”¨æ›´ç»†ç²’åº¦çš„æ¸©åº¦æœç´¢\\n")
                f.write("   - è€ƒè™‘ç±»åˆ«ç‰¹å®šçš„æ ¡å‡†ç­–ç•¥\\n\\n")
            
            if not routing_target.get('achieved', False):
                f.write("3. **ä¼˜åŒ–è·¯ç”±æ€§èƒ½**:\\n")
                f.write("   - åŠ¨æ€è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼\\n")
                f.write("   - å¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è·¯ç”±ç­–ç•¥\\n")
                f.write("   - ç»“åˆæŸ¥è¯¢å†å²è¿›è¡Œè·¯ç”±ä¼˜åŒ–\\n\\n")
        
        f.write("### å®éªŒæ‰©å±•æ–¹å‘\\n")
        f.write("1. **è·¨åŸŸæ³›åŒ–æ€§éªŒè¯**: åœ¨å…¶ä»–é¢†åŸŸæ•°æ®é›†ä¸Šæµ‹è¯•\\n")
        f.write("2. **å®æ—¶æ€§èƒ½ä¼˜åŒ–**: é’ˆå¯¹ç”Ÿäº§ç¯å¢ƒçš„å»¶è¿Ÿä¼˜åŒ–\\n")
        f.write("3. **å¤šè¯­è¨€æ”¯æŒ**: æ‰©å±•åˆ°éè‹±è¯­æŸ¥è¯¢å¤„ç†\\n")
        f.write("4. **å¢é‡å­¦ä¹ **: æ”¯æŒåœ¨çº¿æ¨¡å‹æ›´æ–°\\n\\n")
        
        f.write("---\\n\\n")
        
        f.write("## ğŸ“Š å®éªŒæ•°æ®æ±‡æ€»\\n\\n")
        f.write("### æ–‡ä»¶æ¸…å•\\n")
        f.write("- ğŸ“ `outputs/models/`: è®­ç»ƒå¥½çš„æ¨¡å‹å’Œæ ¡å‡†å™¨\\n")
        f.write("- ğŸ“ `outputs/results/`: è¯¦ç»†å®éªŒç»“æœå’Œåˆ†ææŠ¥å‘Š\\n")
        f.write("- ğŸ“ `outputs/figures/`: å®éªŒå›¾è¡¨å’Œå¯è§†åŒ–\\n")
        f.write("- ğŸ“ `outputs/figures/paper/`: è®ºæ–‡ä¸“ç”¨å›¾è¡¨\\n")
        f.write("- ğŸ“„ `outputs/logs/`: å®Œæ•´çš„å®éªŒæ—¥å¿—è®°å½•\\n\\n")
        
        f.write("### å¤ç°è¯´æ˜\\n")
        f.write("æ‰€æœ‰å®éªŒç»“æœå‡å¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¤ç°ï¼š\\n")
        f.write("```bash\\n")
        f.write("# å®Œæ•´å®éªŒæµç¨‹\\n")
        f.write("python scripts/01_data_preparation.py\\n")
        f.write("python scripts/02_train_baselines.py\\n")
        f.write("python scripts/03_train_modernbert.py\\n")
        f.write("python scripts/04_calibration.py\\n")
        f.write("python scripts/05_evaluation.py\\n")
        f.write("python scripts/06_routing_test.py\\n")
        f.write("python scripts/07_generate_report.py\\n")
        f.write("```\\n\\n")
        
        f.write("---\\n\\n")
        f.write("**å®éªŒå®Œæˆ**  \\n")
        f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n")
    
    logger.info(f"ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š: {report_file}")


def create_experiment_summary(key_metrics, output_dir):
    """åˆ›å»ºå®éªŒæ‘˜è¦JSON"""
    
    summary = {
        'experiment_id': 'experiment1_complexity_classifier',
        'experiment_name': 'å¤æ‚åº¦åˆ†ç±»å™¨æœ‰æ•ˆæ€§éªŒè¯',
        'completion_time': datetime.now().isoformat(),
        'overall_success': key_metrics['experiment_success'],
        'all_targets_achieved': key_metrics['all_targets_achieved'],
        'core_targets': key_metrics['core_targets_achieved'],
        'best_models': key_metrics.get('best_models', {}),
        'key_improvements': key_metrics.get('key_improvements', {}),
        'next_steps': {
            'ready_for_experiment2': key_metrics['experiment_success'],
            'recommended_model': key_metrics.get('best_models', {}).get('best_calibration', {}).get('model', 'N/A'),
            'priority_improvements': []
        }
    }
    
    # æ·»åŠ æ”¹è¿›å»ºè®®
    if not key_metrics['core_targets_achieved'].get('accuracy', {}).get('achieved', False):
        summary['next_steps']['priority_improvements'].append('improve_classification_accuracy')
    
    if not key_metrics['core_targets_achieved'].get('ece', {}).get('achieved', False):
        summary['next_steps']['priority_improvements'].append('enhance_calibration_methods')
    
    if not key_metrics['core_targets_achieved'].get('routing', {}).get('achieved', False):
        summary['next_steps']['priority_improvements'].append('optimize_routing_strategy')
    
    # ä¿å­˜æ‘˜è¦
    summary_file = output_dir / 'EXPERIMENT_SUMMARY.json'
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ç”Ÿæˆå®éªŒæ‘˜è¦: {summary_file}")
    
    return summary


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç”Ÿæˆæœ€ç»ˆå®éªŒæŠ¥å‘Š")
    parser.add_argument('--output-dir', type=str, default='outputs',
                       help='è¾“å‡ºç›®å½•è·¯å¾„')
    args = parser.parse_args()
    
    logger.info("å¼€å§‹ç”Ÿæˆæœ€ç»ˆå®éªŒæŠ¥å‘Š...")
    
    output_dir = Path(args.output_dir)
    
    # åŠ è½½æ‰€æœ‰ç»“æœ
    all_results = load_all_results(output_dir)
    
    if not all_results:
        logger.error("æœªæ‰¾åˆ°ä»»ä½•çœŸå®å®éªŒç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚è¯·å…ˆè¿è¡Œ 02-06 æ­¥éª¤ç”Ÿæˆ outputs/results ä¸‹çš„çœŸå®æ–‡ä»¶ã€‚")
        return
    
    # æå–å…³é”®æŒ‡æ ‡
    key_metrics = extract_key_metrics(all_results)
    
    # åˆ›å»ºè®ºæ–‡å›¾è¡¨
    create_paper_figures(all_results, output_dir)
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    generate_final_report(all_results, key_metrics, output_dir)
    
    # åˆ›å»ºå®éªŒæ‘˜è¦
    experiment_summary = create_experiment_summary(key_metrics, output_dir)
    
    # è¾“å‡ºå…³é”®ç»“æœ
    logger.info("\\n" + "="*60)
    logger.info("ğŸ‰ å®éªŒä¸€ï¼šå¤æ‚åº¦åˆ†ç±»å™¨æœ‰æ•ˆæ€§éªŒè¯ - å®Œæˆ")
    logger.info("="*60)
    
    if key_metrics['all_targets_achieved']:
        logger.info("âœ… æ‰€æœ‰æ ¸å¿ƒç›®æ ‡å‡å·²è¾¾æˆï¼")
    elif key_metrics['experiment_success']:
        logger.info("âœ… ä¸»è¦ç›®æ ‡è¾¾æˆï¼Œå®éªŒåŸºæœ¬æˆåŠŸï¼")
    else:
        logger.info("âš ï¸  éƒ¨åˆ†ç›®æ ‡æœªè¾¾æˆï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    # ç›®æ ‡è¾¾æˆè¯¦æƒ…
    accuracy_achieved = key_metrics['core_targets_achieved'].get('accuracy', {}).get('achieved', False)
    ece_achieved = key_metrics['core_targets_achieved'].get('ece', {}).get('achieved', False)
    routing_achieved = key_metrics['core_targets_achieved'].get('routing', {}).get('achieved', False)
    
    logger.info(f"\\næ ¸å¿ƒç›®æ ‡è¾¾æˆæƒ…å†µ:")
    logger.info(f"  åˆ†ç±»å‡†ç¡®ç‡ > 85%: {'âœ…' if accuracy_achieved else 'âŒ'}")
    logger.info(f"  ECE < 0.08: {'âœ…' if ece_achieved else 'âŒ'}")
    logger.info(f"  è·¯ç”±å‡†ç¡®ç‡ > 90%: {'âœ…' if routing_achieved else 'âŒ'}")
    
    # æ¨èåç»­è¡ŒåŠ¨
    logger.info(f"\\næ¨èåç»­è¡ŒåŠ¨:")
    if experiment_summary['next_steps']['ready_for_experiment2']:
        logger.info("âœ… å¯ä»¥å¼€å§‹å®éªŒäºŒï¼šç½®ä¿¡åº¦æ„ŸçŸ¥èåˆæœºåˆ¶éªŒè¯")
    else:
        logger.info("âš ï¸  å»ºè®®ä¼˜åŒ–å½“å‰å®éªŒåå†è¿›è¡Œå®éªŒäºŒ")
    
    logger.info(f"\\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ:")
    logger.info(f"  - æœ€ç»ˆæŠ¥å‘Š: outputs/results/FINAL_EXPERIMENT_REPORT.md")
    logger.info(f"  - å®éªŒæ‘˜è¦: outputs/EXPERIMENT_SUMMARY.json")
    logger.info(f"  - è®ºæ–‡å›¾è¡¨: outputs/figures/paper/")
    
    logger.info("\\nå®éªŒä¸€æŠ¥å‘Šç”Ÿæˆå®Œæ¯•! ğŸ‰")


if __name__ == "__main__":
    main()