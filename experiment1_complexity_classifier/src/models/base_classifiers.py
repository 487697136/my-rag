"""
åŸºçº¿åˆ†ç±»å™¨å®ç°

åŒ…å«ä»¥ä¸‹åŸºçº¿æ¨¡å‹:
1. RandomClassifier: éšæœºåˆ†ç±»å™¨
2. RuleBasedClassifier: åŸºäºè§„åˆ™çš„åˆ†ç±»å™¨  
3. BertClassifier: BERT-baseåˆ†ç±»å™¨
4. RobertaClassifier: RoBERTa-largeåˆ†ç±»å™¨
"""

import os
import random
import re
import torch
import torch.nn as nn
import numpy as np
import logging
from typing import List, Dict, Tuple, Optional, Union
from pathlib import Path
from transformers import (
    AutoTokenizer, AutoModel, AutoConfig,
    BertTokenizer, BertModel,
    RobertaTokenizer, RobertaModel
)
from sklearn.base import BaseEstimator, ClassifierMixin
from tqdm import tqdm

# è®¾ç½®logger
logger = logging.getLogger(__name__)


class RandomClassifier(BaseEstimator, ClassifierMixin):
    """éšæœºåˆ†ç±»å™¨åŸºçº¿
    
    éšæœºé¢„æµ‹æŸ¥è¯¢å¤æ‚åº¦ï¼Œç”¨ä½œæœ€ç®€å•çš„åŸºçº¿å¯¹æ¯”ã€‚
    """
    
    def __init__(self, num_classes: int = 3, random_state: int = 42):
        """
        Args:
            num_classes: ç±»åˆ«æ•°é‡ (zero_hop=0, one_hop=1, multi_hop=2)
            random_state: éšæœºç§å­
        """
        self.num_classes = num_classes
        self.random_state = random_state
        self.classes_ = np.arange(num_classes)
        self.class_names = ['zero_hop', 'one_hop', 'multi_hop']
        
    def fit(self, X, y=None):
        """è®­ç»ƒæ¨¡å‹ï¼ˆéšæœºåˆ†ç±»å™¨æ— éœ€è®­ç»ƒï¼‰"""
        random.seed(self.random_state)
        np.random.seed(self.random_state)
        return self
    
    def predict(self, X) -> np.ndarray:
        """é¢„æµ‹ç±»åˆ«"""
        n_samples = len(X)
        return np.random.choice(self.classes_, size=n_samples)
    
    def predict_proba(self, X) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰"""
        n_samples = len(X)
        # ç”Ÿæˆéšæœºæ¦‚ç‡å¹¶å½’ä¸€åŒ–
        probs = np.random.random((n_samples, self.num_classes))
        probs = probs / probs.sum(axis=1, keepdims=True)
        return probs
    
    def get_logits(self, X) -> np.ndarray:
        """è·å–logitsï¼ˆä»æ¦‚ç‡åæ¨ï¼‰"""
        probs = self.predict_proba(X)
        # æ·»åŠ å°çš„å™ªå£°é¿å…log(0)
        probs = np.clip(probs, 1e-8, 1 - 1e-8)
        return np.log(probs)


class RuleBasedClassifier(BaseEstimator, ClassifierMixin):
    """åŸºäºè§„åˆ™çš„åˆ†ç±»å™¨
    
    ä½¿ç”¨å…³é”®è¯ã€æŸ¥è¯¢é•¿åº¦å’Œè¯­æ³•æ¨¡å¼æ¥åˆ¤æ–­å¤æ‚åº¦ã€‚
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Args:
            config: è§„åˆ™é…ç½®å­—å…¸
        """
        self.config = config or self._default_config()
        self.classes_ = np.array([0, 1, 2])  # zero_hop, one_hop, multi_hop
        self.class_names = ['zero_hop', 'one_hop', 'multi_hop']
        
    def _default_config(self) -> Dict:
        """é»˜è®¤è§„åˆ™é…ç½®"""
        return {
            'keywords': {
                'zero_hop': [
                    'what is', 'define', 'meaning', 'definition',
                    'who is', 'when was', 'where is'
                ],
                'one_hop': [
                    'who', 'when', 'where', 'which', 'how many',
                    'list', 'name', 'identify'
                ],
                'multi_hop': [
                    'compare', 'relationship', 'analyze', 'both',
                    'and', 'or', 'between', 'difference', 'similar'
                ]
            },
            'length_thresholds': {
                'short': 5,   # <= 5 words: likely zero-hop
                'medium': 15, # 6-15 words: likely one-hop  
                'long': 25    # > 15 words: likely multi-hop
            },
            'patterns': {
                'question_words': r'\b(what|who|when|where|why|how|which)\b',
                'comparison': r'\b(compare|versus|vs|difference|similar)\b',
                'multiple_entities': r'\band\b|\bor\b',
                'superlatives': r'\b(most|least|best|worst|largest|smallest)\b'
            }
        }
    
    def fit(self, X, y=None):
        """è®­ç»ƒæ¨¡å‹ï¼ˆè§„åˆ™åˆ†ç±»å™¨æ— éœ€è®­ç»ƒï¼‰"""
        return self
    
    def _extract_features(self, query: str) -> Dict:
        """ä»æŸ¥è¯¢ä¸­æå–ç‰¹å¾"""
        query_lower = query.lower().strip()
        words = query_lower.split()
        
        features = {
            'length': len(words),
            'has_question_word': bool(re.search(self.config['patterns']['question_words'], query_lower)),
            'has_comparison': bool(re.search(self.config['patterns']['comparison'], query_lower)),
            'has_multiple_entities': bool(re.search(self.config['patterns']['multiple_entities'], query_lower)),
            'has_superlatives': bool(re.search(self.config['patterns']['superlatives'], query_lower)),
            'keyword_matches': {
                'zero_hop': sum(1 for kw in self.config['keywords']['zero_hop'] if kw in query_lower),
                'one_hop': sum(1 for kw in self.config['keywords']['one_hop'] if kw in query_lower),
                'multi_hop': sum(1 for kw in self.config['keywords']['multi_hop'] if kw in query_lower)
            }
        }
        
        return features
    
    def _rule_based_predict(self, query: str) -> int:
        """åŸºäºè§„åˆ™é¢„æµ‹å•ä¸ªæŸ¥è¯¢çš„å¤æ‚åº¦"""
        features = self._extract_features(query)
        
        # è§„åˆ™1: å…³é”®è¯åŒ¹é…
        keyword_scores = features['keyword_matches']
        max_keyword_category = max(keyword_scores.items(), key=lambda x: x[1])
        
        # è§„åˆ™2: é•¿åº¦åˆ¤æ–­
        length = features['length']
        if length <= self.config['length_thresholds']['short']:
            length_category = 'zero_hop'
        elif length <= self.config['length_thresholds']['medium']:
            length_category = 'one_hop'
        else:
            length_category = 'multi_hop'
        
        # è§„åˆ™3: è¯­æ³•æ¨¡å¼
        if features['has_comparison'] or features['has_multiple_entities']:
            pattern_category = 'multi_hop'
        elif features['has_question_word']:
            pattern_category = 'one_hop'
        else:
            pattern_category = 'zero_hop'
        
        # ç»¼åˆå†³ç­–ï¼ˆä¼˜å…ˆçº§ï¼šå…³é”®è¯ > è¯­æ³•æ¨¡å¼ > é•¿åº¦ï¼‰
        if max_keyword_category[1] > 0:  # æœ‰å…³é”®è¯åŒ¹é…
            predicted_category = max_keyword_category[0]
        elif features['has_comparison'] or features['has_multiple_entities']:
            predicted_category = 'multi_hop'
        else:
            predicted_category = length_category
        
        # æ˜ å°„åˆ°æ•°å€¼æ ‡ç­¾
        category_mapping = {'zero_hop': 0, 'one_hop': 1, 'multi_hop': 2}
        return category_mapping[predicted_category]
    
    def predict(self, X) -> np.ndarray:
        """é¢„æµ‹ç±»åˆ«"""
        predictions = []
        for query in X:
            pred = self._rule_based_predict(query)
            predictions.append(pred)
        return np.array(predictions)
    
    def predict_proba(self, X) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡ï¼ˆåŸºäºç½®ä¿¡åº¦å¯å‘å¼ï¼‰"""
        predictions = self.predict(X)
        n_samples = len(X)
        probs = np.zeros((n_samples, 3))
        
        for i, pred in enumerate(predictions):
            features = self._extract_features(X[i])
            
            # åŸºäºç‰¹å¾åŒ¹é…åº¦è®¡ç®—ç½®ä¿¡åº¦
            keyword_matches = features['keyword_matches']
            max_matches = max(keyword_matches.values())
            
            if max_matches > 0:
                confidence = min(0.9, 0.6 + max_matches * 0.1)
            else:
                confidence = 0.4  # ä½ç½®ä¿¡åº¦
            
            # å°†ç½®ä¿¡åº¦åˆ†é…ç»™é¢„æµ‹ç±»åˆ«ï¼Œå…¶ä½™å¹³å‡åˆ†é…
            probs[i, pred] = confidence
            remaining_prob = (1 - confidence) / 2
            for j in range(3):
                if j != pred:
                    probs[i, j] = remaining_prob
                    
        return probs
    
    def get_logits(self, X) -> np.ndarray:
        """è·å–logits"""
        probs = self.predict_proba(X)
        probs = np.clip(probs, 1e-8, 1 - 1e-8)
        return np.log(probs)


class LocalModelLoader:
    """æœ¬åœ°æ¨¡å‹åŠ è½½å™¨"""
    
    # è®°å½•å·²è§£ææˆåŠŸçš„åœ¨çº¿å€™é€‰ï¼Œé¿å…åœ¨åŒä¸€è¿›ç¨‹ä¸­é‡å¤å°è¯•å¤±è´¥çš„å€™é€‰
    _resolved_online_model_cache: Dict[str, str] = {}

    @staticmethod
    def get_local_model_path(model_name: str, experiment_root: Optional[Path] = None) -> Optional[Path]:
        """è·å–æœ¬åœ°æ¨¡å‹è·¯å¾„"""
        if experiment_root is None:
            # è‡ªåŠ¨æ£€æµ‹å®éªŒæ ¹ç›®å½•
            current_file = Path(__file__)
            experiment_root = current_file.parent.parent.parent
        
        # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–æœ¬åœ°æ¨¡å‹é…ç½®
        config_path = experiment_root / 'config' / 'model_config.yaml'
        if config_path.exists():
            try:
                import yaml
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                
                local_models = config.get('local_models', {})
                if local_models.get('enabled', False):
                    models_config = local_models.get('models', {})
                    if model_name in models_config:
                        local_path = experiment_root / models_config[model_name]['local_path']
                        if local_path.exists():
                            return local_path
            except Exception as e:
                logger.warning(f"è¯»å–æœ¬åœ°æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        
        # å¤‡ç”¨æ–¹æ³•ï¼šæ£€æŸ¥æ ‡å‡†æœ¬åœ°æ¨¡å‹ç›®å½•ï¼ˆåŒ…å« ModernBERT çš„è§„èŒƒåŒ–ç›®å½•ä¸å¸¸è§é”™åå®¹é”™ï¼‰
        models_dir = experiment_root / 'models'
        model_mapping = {
            'bert-base-uncased': 'bert_base_uncased',
            'bert-base-cased': 'bert_base_cased',
            'roberta-large': 'roberta_large',
            # ModernBERT è§„èŒƒåŒ–å‘½åï¼ˆä¼˜å…ˆï¼‰
            'answerdotai/ModernBERT-large': str(Path('modernbert') / 'answerdotai_ModernBERT-large'),
            'ModernBERT-large': str(Path('modernbert') / 'answerdotai_ModernBERT-large'),
        }
        
        # è§„èŒƒåŒ–ï¼šå¯¹ ModernBERT è¿›è¡Œç‰¹åˆ«å¤„ç†ï¼Œå…¼å®¹å¸¸è§æ‹¼å†™è¯¯å·®ç›®å½•
        def _validate_local_path(path: Path) -> Optional[Path]:
            if not path.exists():
                return None
            required_files = [
                'pytorch_model.bin', 'model.safetensors', 'tf_model.h5',
                'model.ckpt.index', 'flax_model.msgpack'
            ]
            return path if any((path / f).exists() for f in required_files) else None

        # å…ˆç”¨ç›´æ¥æ˜ å°„
        if model_name in model_mapping:
            candidate = _validate_local_path(models_dir / model_mapping[model_name])
            if candidate is not None:
                return candidate

        # ModernBERT å®¹é”™è·¯å¾„å€™é€‰ï¼ˆå¤„ç†æ—¥å¿—é‡Œå‡ºç°çš„è¯¯åï¼‰ï¼Œå¹¶åœ¨å¿…è¦æ—¶è¿ç§»åˆ°è§„èŒƒè·¯å¾„
        if 'modernbert' in model_name.lower() or 'answerdotai/modernbert' in model_name.lower():
            canonical = models_dir / 'modernbert' / 'answerdotai_ModernBERT-large'
            typo_candidates = [
                canonical,
                models_dir / 'modernswerdotai_ModernBERT-large',
                models_dir / 'modernbewerdotai_ModernBERT-large',
            ]
            for p in typo_candidates:
                candidate = _validate_local_path(p)
                if candidate is None:
                    continue
                # å‘½ä¸­é”™åç›®å½•ï¼Œä¸”è§„èŒƒç›®å½•ä¸å­˜åœ¨æ—¶ï¼Œå°è¯•è¿ç§»
                if candidate != canonical and _validate_local_path(canonical) is None:
                    try:
                        import shutil
                        canonical.parent.mkdir(parents=True, exist_ok=True)
                        logger.info("æ£€æµ‹åˆ° ModernBERT é”™åç›®å½•ï¼Œæ­£åœ¨è¿ç§» -> %s", canonical)
                        shutil.move(str(candidate), str(canonical))
                        logger.info("è¿ç§»å®Œæˆ: %s -> %s", candidate, canonical)
                        return canonical
                    except Exception as move_err:
                        logger.warning("è¿ç§» ModernBERT ç›®å½•å¤±è´¥ï¼Œä½¿ç”¨åŸè·¯å¾„: %sï¼Œé”™è¯¯: %s", candidate, move_err)
                        return candidate
                # å·²æ˜¯è§„èŒƒç›®å½•æˆ–è§„èŒƒç›®å½•å¯ç”¨
                return candidate
        
        return None
    
    @staticmethod
    def get_huggingface_model_name(model_name: str) -> str:
        """è·å–æ­£ç¡®çš„ Hugging Face æ¨¡å‹åç§°"""
        # ä¸ºå…¼å®¹ä¸åŒç»„ç»‡å‘½åï¼Œä¼˜å…ˆè¿”å›æ›´é€šç”¨çš„ ID
        # æ³¨æ„ï¼šä¿æŒè¿”å›æ—§æ¥å£ä»¥å…¼å®¹æ—§è°ƒç”¨å¤„
        if model_name == 'bert-base-uncased':
            return 'bert-base-uncased'
        if model_name == 'bert-base-cased':
            return 'bert-base-cased'
        if model_name == 'roberta-large':
            return 'roberta-large'
        return model_name

    @staticmethod
    def get_huggingface_model_candidates(model_name: str) -> list:
        """è¿”å›å¯å°è¯•çš„å¤šä¸ª Hugging Face æ¨¡å‹åç§°å€™é€‰ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åˆ—"""
        # æ‰©å±•å€™é€‰åˆ—è¡¨ï¼ŒåŒ…å«æ›´å¤šå˜ä½“å’Œå›é€€é€‰é¡¹
        candidates_map = {
            'bert-base-uncased': [
                'bert-base-uncased',
                'google-bert/bert-base-uncased',
                'distilbert-base-uncased',
                'bert-base-cased'
            ],
            'bert-base-cased': [
                'bert-base-cased',  # æ ‡å‡†åç§°ï¼ˆä¼˜å…ˆï¼‰
                'bert-base-uncased'  # å¤‡é€‰uncasedç‰ˆæœ¬
            ],
            'roberta-large': [
                'roberta-large',  # æ ‡å‡†åç§°ï¼ˆä¼˜å…ˆï¼‰
                'FacebookAI/roberta-large',  # ç»„ç»‡å‰ç¼€ç‰ˆæœ¬
                'roberta-base',  # è½»é‡ç‰ˆå›é€€
                'microsoft/DialoGPT-medium'  # å…¼å®¹æ€§å¤‡é€‰
            ]
        }
        
        # åŸºç¡€å¤„ç†
        base_name = LocalModelLoader.get_huggingface_model_name(model_name)
        
        # è·å–å€™é€‰åˆ—è¡¨ï¼Œå¦‚æœä¸åœ¨æ˜ å°„ä¸­ï¼Œä½¿ç”¨é»˜è®¤å€™é€‰
        if model_name in candidates_map:
            return candidates_map[model_name]
        elif base_name in candidates_map:
            return candidates_map[base_name]
        else:
            # é»˜è®¤å€™é€‰ï¼šåŸåç§° + åŸºç¡€åç§°
            return [model_name, base_name] if model_name != base_name else [model_name]
    
    @staticmethod
    def load_model_from_path_or_name(
        model_name_or_path: str,
        config_class=AutoConfig,
        model_class=AutoModel,
        tokenizer_class=AutoTokenizer,
        fallback_to_online: bool = True
    ):
        """ä»æœ¬åœ°è·¯å¾„æˆ–åœ¨çº¿åŠ è½½æ¨¡å‹"""
        # é¦–å…ˆå°è¯•æœ¬åœ°è·¯å¾„
        local_path = LocalModelLoader.get_local_model_path(model_name_or_path)
        
        if local_path is not None:
            logger.info(f"ğŸ  ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_path}")
            try:
                # å¯¹äºåŒ…å«è‡ªå®šä¹‰ä»£ç çš„æ¨¡å‹ï¼ˆå¦‚ ModernBERTï¼‰ï¼Œéœ€è¦å¼€å¯ trust_remote_code
                config = config_class.from_pretrained(
                    str(local_path), local_files_only=True, trust_remote_code=True
                )
                model = model_class.from_pretrained(
                    str(local_path), local_files_only=True, trust_remote_code=True
                )
                # ç»Ÿä¸€ä½¿ç”¨ AutoTokenizerï¼Œç¡®ä¿ä¼˜å…ˆåŠ è½½ Fast ç‰ˆå¹¶æ”¯æŒè‡ªå®šä¹‰å®ç°
                tokenizer = AutoTokenizer.from_pretrained(
                    str(local_path), local_files_only=True, trust_remote_code=True, use_fast=True
                )
                return config, model, tokenizer
            except Exception as e:
                logger.warning(f"æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                if not fallback_to_online:
                    raise
        
        # å›é€€åˆ°åœ¨çº¿ä¸‹è½½
        if fallback_to_online:
            # ä¾æ¬¡å°è¯•å€™é€‰æ¨¡å‹åç§°
            cache_dir = os.environ.get("HF_HOME")
            offline_mode = os.environ.get("TRANSFORMERS_OFFLINE") == "1"
            last_error: Optional[Exception] = None
            candidates = LocalModelLoader.get_huggingface_model_candidates(
                model_name_or_path
            )

            # ä¼˜å…ˆä½¿ç”¨å·²è§£æçš„åœ¨çº¿å€™é€‰ï¼Œé¿å…æ¯ä¸ªfoldåå¤å°è¯•å¤±è´¥é€‰é¡¹
            if model_name_or_path in LocalModelLoader._resolved_online_model_cache:
                resolved = LocalModelLoader._resolved_online_model_cache[model_name_or_path]
                candidates = [resolved] + [c for c in candidates if c != resolved]
            
            logger.info(f"å°è¯•åœ¨çº¿åŠ è½½æ¨¡å‹ {model_name_or_path}ï¼Œå€™é€‰åˆ—è¡¨: {candidates}")
            
            for idx, hf_model_name in enumerate(candidates):
                try:
                    logger.info(f"â˜ï¸ å°è¯•åœ¨çº¿æ¨¡å‹ [{idx+1}/{len(candidates)}]: {hf_model_name}")
                    
                    # å¢åŠ è¶…æ—¶å’Œé‡è¯•è®¾ç½®ï¼Œå¤„ç†ç½‘ç»œé—®é¢˜
                    config = config_class.from_pretrained(
                        hf_model_name,
                        cache_dir=cache_dir,
                        resume_download=True,
                        local_files_only=offline_mode,
                        trust_remote_code=True,
                        proxies=None,  # ç¡®ä¿ä¸ä½¿ç”¨ä»£ç†
                        use_auth_token=False,  # ä¸ä½¿ç”¨è®¤è¯
                        force_download=False  # å…è®¸ä½¿ç”¨ç¼“å­˜
                    )
                    model = model_class.from_pretrained(
                        hf_model_name,
                        config=config,
                        cache_dir=cache_dir,
                        resume_download=True,
                        local_files_only=offline_mode,
                        trust_remote_code=True,
                        proxies=None,
                        use_auth_token=False,
                        force_download=False
                    )
                    # ç»Ÿä¸€ä½¿ç”¨ AutoTokenizerï¼Œä¼˜å…ˆ Fast ç‰ˆå¹¶æ”¯æŒè‡ªå®šä¹‰å®ç°
                    tokenizer = AutoTokenizer.from_pretrained(
                        hf_model_name,
                        cache_dir=cache_dir,
                        resume_download=True,
                        local_files_only=offline_mode,
                        trust_remote_code=True,
                        proxies=None,
                        use_auth_token=False,
                        force_download=False,
                        use_fast=True,
                    )
                    logger.info(f"âœ… æˆåŠŸåŠ è½½åœ¨çº¿æ¨¡å‹: {hf_model_name}")
                    # è®°å½•æˆåŠŸè§£æçš„å€™é€‰ï¼Œåç»­åŒåè¯·æ±‚ç›´æ¥ä½¿ç”¨
                    LocalModelLoader._resolved_online_model_cache[model_name_or_path] = hf_model_name
                    return config, model, tokenizer
                    
                except Exception as e:  # å°è¯•ä¸‹ä¸€ä¸ªå€™é€‰
                    last_error = e
                    logger.warning(f"âŒ åœ¨çº¿åŠ è½½å¤±è´¥ [{idx+1}/{len(candidates)}] {hf_model_name}: {type(e).__name__}: {str(e)[:200]}")
                    
                    # å¯¹äºç½‘ç»œé”™è¯¯ï¼Œå¿«é€Ÿå°è¯•ä¸‹ä¸€ä¸ªå€™é€‰
                    if any(keyword in str(e).lower() for keyword in ['network', 'connection', 'resolve', 'timeout', 'httpsconnection']):
                        logger.info(f"æ£€æµ‹åˆ°ç½‘ç»œé—®é¢˜ï¼Œè·³åˆ°ä¸‹ä¸€ä¸ªå€™é€‰æ¨¡å‹...")
                        continue
                    else:
                        # éç½‘ç»œé”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
                        logger.error(f"æ¨¡å‹ç‰¹å®šé”™è¯¯ {hf_model_name}: {e}")
                        continue
            
            # å…¨éƒ¨å¤±è´¥ - æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè§£å†³å»ºè®®
            error_msg = f"""æ— æ³•åŠ è½½æ¨¡å‹ '{model_name_or_path}'ï¼Œå·²å°è¯•æ‰€æœ‰å€™é€‰: {candidates}
            
å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š
1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
2. ä½¿ç”¨ --use_online_models å‚æ•°ç›´æ¥åœ¨çº¿æ¨¡å¼
3. é¢„å…ˆä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°: python scripts/download_models.py
4. è®¾ç½®ç¯å¢ƒå˜é‡ HF_ENDPOINT=https://hf-mirror.com ï¼ˆä½¿ç”¨é•œåƒï¼‰

æœ€åé”™è¯¯: {last_error}"""
            raise RuntimeError(error_msg)
        else:
            raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹ {model_name_or_path}ï¼šæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ä¸”ç¦ç”¨åœ¨çº¿å›é€€")

    @staticmethod
    def load_tokenizer_only(
        model_name_or_path: str,
        tokenizer_class=AutoTokenizer,
        fallback_to_online: bool = True
    ):
        """ä»…åŠ è½½åˆ†è¯å™¨ï¼ˆæœ¬åœ°ä¼˜å…ˆ + åœ¨çº¿å€™é€‰ + è§£æç¼“å­˜ï¼‰

        è¯´æ˜ï¼š
        - ä¸€äº›è®­ç»ƒæµç¨‹ä»…éœ€è¦åˆ†è¯å™¨ï¼Œæ— éœ€æå‰åŠ è½½å®Œæ•´æ¨¡å‹ï¼›
        - å¤ç”¨å€™é€‰è§£æä¸ç¼“å­˜é€»è¾‘ï¼Œé¿å…åœ¨æ¯ä¸ªfoldä¸­é‡å¤å°è¯•å¤±è´¥å€™é€‰ï¼›
        """
        # å…ˆæŸ¥æ‰¾æœ¬åœ°è·¯å¾„
        local_path = LocalModelLoader.get_local_model_path(model_name_or_path)
        if local_path is not None:
            logger.info(f"ğŸ  ä½¿ç”¨æœ¬åœ°åˆ†è¯å™¨: {local_path}")
            try:
                # ç»Ÿä¸€ä½¿ç”¨ AutoTokenizerï¼Œä¼˜å…ˆ Fast ç‰ˆå¹¶æ”¯æŒè‡ªå®šä¹‰å®ç°
                tokenizer = AutoTokenizer.from_pretrained(
                    str(local_path), local_files_only=True, trust_remote_code=True, use_fast=True
                )
                return tokenizer
            except Exception as e:
                logger.warning(f"æœ¬åœ°åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
                if not fallback_to_online:
                    raise
        
        if not fallback_to_online:
            raise RuntimeError(
                f"æ— æ³•åŠ è½½åˆ†è¯å™¨ {model_name_or_path}ï¼šæœ¬åœ°ä¸å­˜åœ¨ä¸”ç¦ç”¨åœ¨çº¿å›é€€"
            )

        # åœ¨çº¿å€™é€‰ + è§£æç¼“å­˜
        cache_dir = os.environ.get("HF_HOME")
        offline_mode = os.environ.get("TRANSFORMERS_OFFLINE") == "1"
        last_error: Optional[Exception] = None
        candidates = LocalModelLoader.get_huggingface_model_candidates(
            model_name_or_path
        )
        if model_name_or_path in LocalModelLoader._resolved_online_model_cache:
            resolved = LocalModelLoader._resolved_online_model_cache[model_name_or_path]
            candidates = [resolved] + [c for c in candidates if c != resolved]

        logger.info(
            f"å°è¯•åœ¨çº¿åŠ è½½åˆ†è¯å™¨ {model_name_or_path}ï¼Œå€™é€‰åˆ—è¡¨: {candidates}"
        )
        for idx, hf_model_name in enumerate(candidates):
            try:
                logger.info(
                    f"â˜ï¸ å°è¯•åœ¨çº¿åˆ†è¯å™¨ [{idx+1}/{len(candidates)}]: {hf_model_name}"
                )
                # ç»Ÿä¸€ä½¿ç”¨ AutoTokenizerï¼Œä¼˜å…ˆ Fast ç‰ˆå¹¶æ”¯æŒè‡ªå®šä¹‰å®ç°
                tokenizer = AutoTokenizer.from_pretrained(
                    hf_model_name,
                    cache_dir=cache_dir,
                    resume_download=True,
                    local_files_only=offline_mode,
                    trust_remote_code=True,
                    proxies=None,
                    use_auth_token=False,
                    force_download=False,
                    use_fast=True,
                )
                logger.info(f"âœ… æˆåŠŸåŠ è½½åœ¨çº¿åˆ†è¯å™¨: {hf_model_name}")
                LocalModelLoader._resolved_online_model_cache[model_name_or_path] = (
                    hf_model_name
                )
                return tokenizer
            except Exception as e:
                last_error = e
                logger.warning(
                    f"âŒ åœ¨çº¿åˆ†è¯å™¨å¤±è´¥ [{idx+1}/{len(candidates)}] {hf_model_name}: "
                    f"{type(e).__name__}: {str(e)[:200]}"
                )
                continue
        raise RuntimeError(
            f"æ— æ³•åŠ è½½åˆ†è¯å™¨ '{model_name_or_path}'ï¼Œå·²å°è¯•æ‰€æœ‰å€™é€‰: {candidates}\næœ€åé”™è¯¯: {last_error}"
        )


class TransformerClassifier(nn.Module):
    """TransformeråŸºç¡€åˆ†ç±»å™¨ç±»"""
    
    def __init__(
        self,
        model_name: str,
        num_classes: int = 3,
        hidden_size: Optional[int] = None,
        dropout: float = 0.1,
        classifier_hidden_layers: Optional[List[int]] = None,
        use_local_model: bool = True,
        fallback_to_online: bool = True
    ):
        """
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
            hidden_size: éšè—å±‚å¤§å°
            dropout: Dropoutç‡
            classifier_hidden_layers: åˆ†ç±»å¤´éšè—å±‚å¤§å°åˆ—è¡¨
            use_local_model: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹
            fallback_to_online: å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œæ˜¯å¦å›é€€åˆ°åœ¨çº¿ä¸‹è½½
        """
        super().__init__()
        
        self.model_name = model_name
        self.num_classes = num_classes
        self.hidden_size = hidden_size
        self.use_local_model = use_local_model
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ”¯æŒæœ¬åœ°ä¼˜å…ˆç­–ç•¥ï¼‰
        if use_local_model:
            self.config, self.transformer, self.tokenizer = LocalModelLoader.load_model_from_path_or_name(
                model_name,
                fallback_to_online=fallback_to_online
            )
        else:
            # ä¼ ç»Ÿåœ¨çº¿åŠ è½½æ–¹å¼
            cache_dir = os.environ.get("HF_HOME")
            offline_mode = os.environ.get("TRANSFORMERS_OFFLINE") == "1"
            
            self.config = AutoConfig.from_pretrained(
                model_name, 
                cache_dir=cache_dir, 
                resume_download=True, 
                local_files_only=offline_mode
            )
            self.transformer = AutoModel.from_pretrained(
                model_name, 
                config=self.config, 
                cache_dir=cache_dir, 
                resume_download=True, 
                local_files_only=offline_mode
            )
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                cache_dir=cache_dir, 
                resume_download=True, 
                local_files_only=offline_mode,
                trust_remote_code=True,
                use_fast=True,
            )
        
        # æ¨æ–­éšè—ç»´åº¦ï¼šä¼˜å…ˆä½¿ç”¨æ˜¾å¼ä¼ å…¥ï¼Œå¦åˆ™ä»é…ç½®è‡ªåŠ¨è·å–
        effective_hidden_size = (
            self.hidden_size
            if self.hidden_size is not None
            else getattr(self.config, "hidden_size", None)
        )
        if effective_hidden_size is None:
            effective_hidden_size = 768
        self.hidden_size = effective_hidden_size

        # åˆ†ç±»å¤´
        if classifier_hidden_layers:
            layers = []
            input_size = effective_hidden_size
            for hidden_dim in classifier_hidden_layers:
                layers.extend([
                    nn.Linear(input_size, hidden_dim),
                    nn.LayerNorm(hidden_dim),
                    nn.GELU(),
                    nn.Dropout(dropout)
                ])
                input_size = hidden_dim
            layers.append(nn.Linear(input_size, num_classes))
            self.classifier = nn.Sequential(*layers)
        else:
            self.classifier = nn.Sequential(
                nn.Dropout(dropout),
                nn.Linear(effective_hidden_size, num_classes)
            )
        
        # åˆå§‹åŒ–åˆ†ç±»å™¨æƒé‡
        self._init_classifier_weights()
    
    def _init_classifier_weights(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨æƒé‡"""
        for module in self.classifier:
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, std=0.02)
                nn.init.zeros_(module.bias)
    
    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        """å‰å‘ä¼ æ’­"""
        # å…¼å®¹ä¸åŒæ¶æ„ï¼šéƒ¨åˆ†æ¨¡å‹ï¼ˆå¦‚ ModernBERTï¼‰ä¸æ¥å— token_type_ids å‚æ•°
        # ä¸ºæœ€å¤§å…¼å®¹æ€§ï¼Œè¿™é‡Œä¸å‘åº•å±‚æ¨¡å‹ä¼ é€’ token_type_idsã€‚
        outputs = self.transformer(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # ä½¿ç”¨[CLS] tokençš„è¡¨ç¤º
        pooled_output = outputs.last_hidden_state[:, 0]  # [batch_size, hidden_size]
        logits = self.classifier(pooled_output)
        
        return {
            'logits': logits,
            'hidden_states': outputs.last_hidden_state,
            'pooled_output': pooled_output
        }


class BertClassifier(BaseEstimator, ClassifierMixin):
    """BERT-baseåˆ†ç±»å™¨åŒ…è£…ç±»"""
    
    def __init__(
        self,
        model_name: str = "bert-base-cased",
        num_classes: int = 3,
        max_length: int = 512,
        device: str = "auto",
        use_local_model: bool = True,
        fallback_to_online: bool = True
    ):
        self.model_name = model_name
        self.num_classes = num_classes
        self.max_length = max_length
        self.device = device if device != "auto" else ("cuda" if torch.cuda.is_available() else "cpu")
        self.use_local_model = use_local_model
        self.fallback_to_online = fallback_to_online
        
        self.classes_ = np.array([0, 1, 2])
        self.class_names = ['zero_hop', 'one_hop', 'multi_hop']
        
        # æ¨¡å‹å°†åœ¨fitæ—¶åˆå§‹åŒ–
        self.model = None
        self.tokenizer = None
        self.is_fitted = False
    
    def fit(self, X, y, learning_rate=2e-5, batch_size=16, max_epochs=5,
            validation_data=None, early_stopping=False, patience=2,
            weight_decay: float = 0.0, **kwargs):
        """è®­ç»ƒæ¨¡å‹ - å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        from torch.utils.data import DataLoader, TensorDataset
        from torch.optim import AdamW
        from sklearn.metrics import accuracy_score
        import torch.nn.functional as F
        
        logger.info(f"å¼€å§‹è®­ç»ƒ {self.model_name}")
        
        # ä½¿ç”¨æœ¬åœ°ä¼˜å…ˆç­–ç•¥ä»…åŠ è½½åˆ†è¯å™¨ï¼ˆé¿å…é‡å¤åŠ è½½å®Œæ•´æ¨¡å‹ï¼‰
        try:
            self.tokenizer = LocalModelLoader.load_tokenizer_only(
                self.model_name,
                fallback_to_online=self.fallback_to_online
            )
        except Exception as e:
            logger.error(f"åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = TransformerClassifier(
            model_name=self.model_name,
            num_classes=self.num_classes,
            hidden_size=None,
            use_local_model=self.use_local_model,
            fallback_to_online=self.fallback_to_online
        )
        self.model.to(self.device)
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        train_encodings = self.tokenizer(
            X, truncation=True, padding=True, max_length=self.max_length, return_tensors="pt"
        )
        train_labels = torch.tensor(y, dtype=torch.long)
        train_dataset = TensorDataset(
            train_encodings['input_ids'].to(self.device),
            train_encodings['attention_mask'].to(self.device),
            train_labels.to(self.device)
        )
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # å‡†å¤‡éªŒè¯æ•°æ®ï¼ˆå¦‚æœæä¾›ï¼‰
        val_loader = None
        if validation_data is not None:
            X_val, y_val = validation_data
            val_encodings = self.tokenizer(
                X_val, truncation=True, padding=True, max_length=self.max_length, return_tensors="pt"
            )
            val_labels = torch.tensor(y_val, dtype=torch.long)
            val_dataset = TensorDataset(
                val_encodings['input_ids'].to(self.device),
                val_encodings['attention_mask'].to(self.device),
                val_labels.to(self.device)
            )
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆä½¿ weight_decay ç”Ÿæ•ˆï¼‰
        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        logger.info(
            "BertClassifier ä¼˜åŒ–å™¨: AdamW | lr=%.2e, weight_decay=%s, batch_size=%s",
            learning_rate, weight_decay, batch_size
        )
        
        # è®­ç»ƒå¾ªç¯
        self.model.train()
        best_val_acc = 0.0
        patience_counter = 0
        
        # è®­ç»ƒå†å²è®°å½•
        history = {
            'train_loss': [],
            'train_acc': [],
            'val_acc': [],
            'epochs_completed': 0
        }
        
        # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆå‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼‰
        accumulation_steps = 2 if batch_size >= 16 else 1
        
        for epoch in range(max_epochs):
            total_loss = 0
            correct_predictions = 0
            total_predictions = 0
            
            # æ·»åŠ è¿›åº¦æ¡
            logger.info(f'Epoch {epoch+1}/{max_epochs} - å¼€å§‹è®­ç»ƒ...')
            train_loader_with_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{max_epochs}', leave=False)
            
            # æ¢¯åº¦ç´¯ç§¯è®¡æ•°å™¨
            step_count = 0
            
            for batch_idx, batch in enumerate(train_loader):
                input_ids, attention_mask, labels = batch
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                loss = F.cross_entropy(outputs['logits'], labels)
                
                # å½’ä¸€åŒ–æŸå¤±ï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
                loss = loss / accumulation_steps
                loss.backward()
                
                total_loss += loss.item() * accumulation_steps
                
                # è®¡ç®—å‡†ç¡®ç‡
                with torch.no_grad():
                    _, predicted = torch.max(outputs['logits'], 1)
                    correct_predictions += (predicted == labels).sum().item()
                    total_predictions += labels.size(0)
                
                # æ¢¯åº¦ç´¯ç§¯å’Œä¼˜åŒ–å™¨æ­¥éª¤
                step_count += 1
                if step_count % accumulation_steps == 0:
                    optimizer.step()
                    optimizer.zero_grad()
                    
                    # å®šæœŸæ¸…ç†GPUç¼“å­˜
                    if batch_idx % 50 == 0:
                        torch.cuda.empty_cache()
            
            # å¤„ç†æœ€åå‰©ä½™çš„æ¢¯åº¦
            if step_count % accumulation_steps != 0:
                optimizer.step()
                optimizer.zero_grad()
            
            train_acc = correct_predictions / total_predictions
            avg_loss = total_loss / len(train_loader)
            logger.info(f'Epoch {epoch+1}/{max_epochs} å®Œæˆ - Loss: {avg_loss:.4f}, Acc: {train_acc:.4f}')
            
            # è®°å½•è®­ç»ƒå†å²
            history['train_loss'].append(avg_loss)
            history['train_acc'].append(train_acc)
            
            # éªŒè¯
            val_acc = 0.0
            if val_loader is not None:
                self.model.eval()
                val_correct = 0
                val_total = 0
                
                with torch.no_grad():
                    for batch in val_loader:
                        input_ids, attention_mask, labels = batch
                        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                        _, predicted = torch.max(outputs['logits'], 1)
                        val_correct += (predicted == labels).sum().item()
                        val_total += labels.size(0)
                
                val_acc = val_correct / val_total
                self.model.train()
                
                # è®°å½•éªŒè¯å†å²
                history['val_acc'].append(val_acc)
                
                # æ—©åœæœºåˆ¶
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if early_stopping and patience_counter >= patience:  # æ—©åœ
                        logger.info(f"æ—©åœäºepoch {epoch+1}")
                        break
            
            logger.info(f"Epoch {epoch+1}/{max_epochs}: Loss={avg_loss:.4f}, "
                       f"Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}")
            
            # æ¯ä¸ª epoch ç»“æŸæ¸…ç†ä¸€æ¬¡æ˜¾å­˜ï¼Œç¼“è§£ç¢ç‰‡åŒ–
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        
        # æ›´æ–°å®Œæˆçš„epochæ•°
        history['epochs_completed'] = epoch + 1
        
        self.is_fitted = True
        
        # è¿”å›è®­ç»ƒå†å²
        return history
    
    def fit_with_params(self, X, y, params):
        """ä½¿ç”¨å‚æ•°å­—å…¸è®­ç»ƒæ¨¡å‹"""
        training_params = {
            'learning_rate': params.get('learning_rate', 2e-5),
            'batch_size': params.get('batch_size', 16),
            'max_epochs': params.get('max_epochs', 5),
            'validation_data': params.get('validation_data'),
            'early_stopping': params.get('early_stopping', False),
            'patience': params.get('patience', 2),
            'weight_decay': params.get('weight_decay', 0.0),
        }
        return self.fit(X, y, **training_params)
    
    def _tokenize_batch(self, texts: List[str]) -> Dict[str, torch.Tensor]:
        """æ‰¹é‡åˆ†è¯"""
        encoded = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=self.max_length,
            return_tensors="pt"
        )
        return {k: v.to(self.device) for k, v in encoded.items()}
    
    def predict(self, X) -> np.ndarray:
        """é¢„æµ‹ç±»åˆ«"""
        logits = self.get_logits(X)
        return np.argmax(logits, axis=1)
    
    def predict_proba(self, X) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡"""
        logits = self.get_logits(X)
        return torch.softmax(torch.tensor(logits), dim=1).numpy()
    
    def get_logits(self, X) -> np.ndarray:
        """è·å–logits"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
        
        self.model.eval()
        all_logits = []
        
        batch_size = 32
        with torch.no_grad():
            for i in range(0, len(X), batch_size):
                batch_texts = X[i:i + batch_size]
                inputs = self._tokenize_batch(batch_texts)
                
                outputs = self.model(**inputs)
                logits = outputs['logits'].cpu().numpy()
                all_logits.append(logits)
        
        return np.concatenate(all_logits, axis=0)


class RobertaClassifier(BertClassifier):
    """RoBERTa-largeåˆ†ç±»å™¨åŒ…è£…ç±»"""
    
    def __init__(
        self,
        model_name: str = "roberta-large",
        num_classes: int = 3,
        max_length: int = 512,
        device: str = "auto",
        use_local_model: bool = True,
        fallback_to_online: bool = True
    ):
        super().__init__(model_name, num_classes, max_length, device, use_local_model, fallback_to_online)
    
    def fit(self, X, y, learning_rate=1e-5, batch_size=16, max_epochs=5,
            validation_data=None, early_stopping=False, patience=2,
            weight_decay: float = 0.0, **kwargs):
        """è®­ç»ƒæ¨¡å‹ - å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ˆå¸¦OOMè‡ªåŠ¨æ¢å¤ï¼‰"""
        from torch.utils.data import DataLoader, TensorDataset
        from torch.optim import AdamW
        from sklearn.metrics import accuracy_score
        import torch.nn.functional as F
        
        logger.info(f"å¼€å§‹è®­ç»ƒ {self.model_name} (åˆå§‹batch_size={batch_size})")
        
        # OOMæ¢å¤æœºåˆ¶ï¼šé€’å½’é‡è¯•ï¼Œæ¯æ¬¡å‡å°batch_size
        def _try_training_with_batch_size(current_batch_size, retry_count=0):
            if current_batch_size < 2:
                raise RuntimeError("æ‰¹æ¬¡å¤§å°å·²å‡å°‘åˆ°æœ€å°å€¼ï¼Œä»ç„¶OOMï¼Œè¯·é‡Šæ”¾æ›´å¤šæ˜¾å­˜æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
            if retry_count > 3:
                raise RuntimeError("OOMé‡è¯•æ¬¡æ•°è¿‡å¤šï¼Œè®­ç»ƒå¤±è´¥")
            
            try:
                return self._actual_fit(X, y, learning_rate, current_batch_size, max_epochs,
                                      validation_data, early_stopping, patience, weight_decay)
            except RuntimeError as e:
                if "out of memory" in str(e).lower() or "cuda" in str(e).lower():
                    logger.warning(f"OOMé”™è¯¯ï¼Œbatch_sizeä»{current_batch_size}å‡å°‘åˆ°{current_batch_size//2}")
                    # æ¸…ç†æ˜¾å­˜
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.reset_peak_memory_stats()
                    # é€’å½’é‡è¯•ï¼Œæ‰¹æ¬¡å¤§å°å‡åŠ
                    return _try_training_with_batch_size(current_batch_size // 2, retry_count + 1)
                else:
                    # éOOMé”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    raise
        
        return _try_training_with_batch_size(batch_size)
    
    def _actual_fit(self, X, y, learning_rate, batch_size, max_epochs,
                   validation_data, early_stopping, patience, weight_decay: float = 0.0):
        """å®é™…çš„è®­ç»ƒé€»è¾‘ï¼ˆä»åŸfitæ–¹æ³•æå–ï¼‰"""
        from torch.utils.data import DataLoader, TensorDataset
        from torch.optim import AdamW
        import torch.nn.functional as F
        
        logger.info(f"å®é™…è®­ç»ƒä½¿ç”¨batch_size={batch_size}")
        
        # ä½¿ç”¨æœ¬åœ°ä¼˜å…ˆç­–ç•¥ä»…åŠ è½½åˆ†è¯å™¨ï¼ˆé¿å…é‡å¤åŠ è½½å®Œæ•´æ¨¡å‹ï¼‰
        try:
            self.tokenizer = LocalModelLoader.load_tokenizer_only(
                self.model_name,
                fallback_to_online=self.fallback_to_online
            )
        except Exception as e:
            logger.error(f"åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = TransformerClassifier(
            model_name=self.model_name,
            num_classes=self.num_classes,
            hidden_size=None,
            use_local_model=self.use_local_model,
            fallback_to_online=self.fallback_to_online
        )
        self.model.to(self.device)
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        train_encodings = self.tokenizer(
            X, truncation=True, padding=True, max_length=self.max_length, return_tensors="pt"
        )
        train_labels = torch.tensor(y, dtype=torch.long)
        train_dataset = TensorDataset(
            train_encodings['input_ids'].to(self.device),
            train_encodings['attention_mask'].to(self.device),
            train_labels.to(self.device)
        )
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # å‡†å¤‡éªŒè¯æ•°æ®ï¼ˆå¦‚æœæä¾›ï¼‰
        val_loader = None
        if validation_data is not None:
            X_val, y_val = validation_data
            val_encodings = self.tokenizer(
                X_val, truncation=True, padding=True, max_length=self.max_length, return_tensors="pt"
            )
            val_labels = torch.tensor(y_val, dtype=torch.long)
            val_dataset = TensorDataset(
                val_encodings['input_ids'].to(self.device),
                val_encodings['attention_mask'].to(self.device),
                val_labels.to(self.device)
            )
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆä½¿ weight_decay ç”Ÿæ•ˆï¼‰
        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        logger.info(
            "RobertaClassifier ä¼˜åŒ–å™¨: AdamW | lr=%.2e, weight_decay=%s, batch_size=%s",
            learning_rate, weight_decay, batch_size
        )
        
        # è®­ç»ƒå¾ªç¯
        self.model.train()
        best_val_acc = 0.0
        patience_counter = 0
        
        # è®­ç»ƒå†å²è®°å½•ï¼ˆä¸ BertClassifier å¯¹é½ï¼Œä¾›ç»˜å›¾ä½¿ç”¨ï¼‰
        history = {
            'train_loss': [],
            'train_acc': [],
            'val_acc': [],
            'epochs_completed': 0
        }
        
        # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆå‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼‰
        accumulation_steps = 2 if batch_size >= 16 else 1
        
        for epoch in range(max_epochs):
            total_loss = 0
            correct_predictions = 0
            total_predictions = 0
            
            # æ·»åŠ è¿›åº¦æ¡
            logger.info(f'Epoch {epoch+1}/{max_epochs} - å¼€å§‹è®­ç»ƒ...')
            train_loader_with_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{max_epochs}', leave=False)
            
            # æ¢¯åº¦ç´¯ç§¯è®¡æ•°å™¨
            step_count = 0
            
            for batch_idx, batch in enumerate(train_loader):
                input_ids, attention_mask, labels = batch
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                loss = F.cross_entropy(outputs['logits'], labels)
                
                # å½’ä¸€åŒ–æŸå¤±ï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
                loss = loss / accumulation_steps
                loss.backward()
                
                total_loss += loss.item() * accumulation_steps
                
                # è®¡ç®—å‡†ç¡®ç‡
                with torch.no_grad():
                    _, predicted = torch.max(outputs['logits'], 1)
                    correct_predictions += (predicted == labels).sum().item()
                    total_predictions += labels.size(0)
                
                # æ¢¯åº¦ç´¯ç§¯å’Œä¼˜åŒ–å™¨æ­¥éª¤
                step_count += 1
                if step_count % accumulation_steps == 0:
                    optimizer.step()
                    optimizer.zero_grad()
                    
                    # å®šæœŸæ¸…ç†GPUç¼“å­˜
                    if batch_idx % 50 == 0:
                        torch.cuda.empty_cache()
            
            # å¤„ç†æœ€åå‰©ä½™çš„æ¢¯åº¦
            if step_count % accumulation_steps != 0:
                optimizer.step()
                optimizer.zero_grad()
            
            train_acc = correct_predictions / total_predictions
            avg_loss = total_loss / len(train_loader)
            logger.info(f'Epoch {epoch+1}/{max_epochs} å®Œæˆ - Loss: {avg_loss:.4f}, Acc: {train_acc:.4f}')
            
            # è®°å½•è®­ç»ƒå†å²
            history['train_loss'].append(avg_loss)
            history['train_acc'].append(train_acc)
            
            # éªŒè¯
            val_acc = 0.0
            if val_loader is not None:
                self.model.eval()
                val_correct = 0
                val_total = 0
                
                with torch.no_grad():
                    for batch in val_loader:
                        input_ids, attention_mask, labels = batch
                        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                        _, predicted = torch.max(outputs['logits'], 1)
                        val_correct += (predicted == labels).sum().item()
                        val_total += labels.size(0)
                
                val_acc = val_correct / val_total
                self.model.train()
                
                # è®°å½•éªŒè¯å†å²
                history['val_acc'].append(val_acc)
                
                # æ—©åœæœºåˆ¶
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if early_stopping and patience_counter >= patience:  # æ—©åœ
                        logger.info(f"æ—©åœäºepoch {epoch+1}")
                        break
            
            logger.info(f"Epoch {epoch+1}/{max_epochs}: Loss={avg_loss:.4f}, "
                       f"Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}")
            
            # æ¯ä¸ª epoch ç»“æŸæ¸…ç†ä¸€æ¬¡æ˜¾å­˜ï¼Œç¼“è§£ç¢ç‰‡åŒ–
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        self.is_fitted = True
        history['epochs_completed'] = epoch + 1
        return history
    
    def fit_with_params(self, X, y, params):
        """ä½¿ç”¨å‚æ•°å­—å…¸è®­ç»ƒæ¨¡å‹"""
        training_params = {
            'learning_rate': params.get('learning_rate', 1e-5),  # RoBERTaç”¨æ›´å°çš„å­¦ä¹ ç‡
            'batch_size': params.get('batch_size', 16),
            'max_epochs': params.get('max_epochs', 5),
            'validation_data': params.get('validation_data')
        }
        return self.fit(X, y, **training_params)