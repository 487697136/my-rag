"""
é«˜çº§å¤æ‚åº¦åˆ†ç±»å™¨RAGæµç¨‹ç¤ºä¾‹

æœ¬ç¤ºä¾‹æä¾›æ›´é«˜çº§çš„åŠŸèƒ½ï¼š
1. æ”¯æŒä»å¤šç§æ ¼å¼æ–‡ä»¶åŠ è½½æ–‡æ¡£ï¼ˆtxt, json, jsonlï¼‰
2. è‡ªå®šä¹‰å¤æ‚åº¦åˆ†ç±»å™¨é…ç½®
3. è¯¦ç»†çš„æ€§èƒ½åˆ†æå’Œå¯è§†åŒ–
4. æ‰¹é‡æŸ¥è¯¢æµ‹è¯•å’Œç»“æœå¯¹æ¯”
5. ç³»ç»ŸçŠ¶æ€ç›‘æ§å’Œè°ƒè¯•

ä½¿ç”¨æ–¹æ³•ï¼š
python examples/using_complexity_classifier_advanced.py --docs_path ./your_documents/ --output_dir ./results/
"""

import os
import sys
import json
import asyncio
import argparse
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from nano_graphrag import GraphRAG, QueryParam
from nano_graphrag.complexity import ComplexityAwareRouter, ComplexityClassifier
from nano_graphrag._utils import logger

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger.setLevel(logging.INFO)


#------------------------------------------------------------------------------
# æ–‡æ¡£åŠ è½½å™¨
#------------------------------------------------------------------------------

class DocumentLoader:
    """æ–‡æ¡£åŠ è½½å™¨ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    
    @staticmethod
    def load_from_txt(file_path: str) -> List[str]:
        """ä»txtæ–‡ä»¶åŠ è½½æ–‡æ¡£"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # æŒ‰æ®µè½åˆ†å‰²
            documents = [doc.strip() for doc in content.split('\n\n') if doc.strip()]
        return documents
    
    @staticmethod
    def load_from_json(file_path: str) -> List[str]:
        """ä»JSONæ–‡ä»¶åŠ è½½æ–‡æ¡£"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        documents = []
        if isinstance(data, list):
            for item in data:
                if isinstance(item, dict):
                    # æå–æ–‡æœ¬å†…å®¹
                    if 'content' in item:
                        documents.append(item['content'])
                    elif 'text' in item:
                        documents.append(item['text'])
                    elif 'title' in item and 'content' in item:
                        documents.append(f"æ ‡é¢˜ï¼š{item['title']}\n\nå†…å®¹ï¼š{item['content']}")
                elif isinstance(item, str):
                    documents.append(item)
        elif isinstance(data, dict):
            # å•ä¸ªæ–‡æ¡£
            if 'content' in data:
                documents.append(data['content'])
            elif 'text' in data:
                documents.append(data['text'])
        
        return documents
    
    @staticmethod
    def load_from_jsonl(file_path: str) -> List[str]:
        """ä»JSONLæ–‡ä»¶åŠ è½½æ–‡æ¡£"""
        documents = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        item = json.loads(line)
                        if isinstance(item, dict):
                            if 'content' in item:
                                documents.append(item['content'])
                            elif 'text' in item:
                                documents.append(item['text'])
                        elif isinstance(item, str):
                            documents.append(item)
                    except json.JSONDecodeError:
                        continue
        return documents
    
    @staticmethod
    def load_documents(docs_path: str) -> List[str]:
        """ä»è·¯å¾„åŠ è½½æ–‡æ¡£ï¼Œè‡ªåŠ¨æ£€æµ‹æ ¼å¼"""
        docs_path = Path(docs_path)
        documents = []
        
        if docs_path.is_file():
            # å•ä¸ªæ–‡ä»¶
            suffix = docs_path.suffix.lower()
            if suffix == '.txt':
                documents = DocumentLoader.load_from_txt(str(docs_path))
            elif suffix == '.json':
                documents = DocumentLoader.load_from_json(str(docs_path))
            elif suffix == '.jsonl':
                documents = DocumentLoader.load_from_jsonl(str(docs_path))
        elif docs_path.is_dir():
            # ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
            for file_path in docs_path.rglob('*'):
                if file_path.is_file():
                    suffix = file_path.suffix.lower()
                    try:
                        if suffix == '.txt':
                            docs = DocumentLoader.load_from_txt(str(file_path))
                        elif suffix == '.json':
                            docs = DocumentLoader.load_from_json(str(file_path))
                        elif suffix == '.jsonl':
                            docs = DocumentLoader.load_from_jsonl(str(file_path))
                        else:
                            continue
                        documents.extend(docs)
                    except Exception as e:
                        logger.warning(f"åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        
        return documents


#------------------------------------------------------------------------------
# å¤æ‚åº¦åˆ†ç±»å™¨é…ç½®
#------------------------------------------------------------------------------

class ComplexityClassifierConfig:
    """å¤æ‚åº¦åˆ†ç±»å™¨é…ç½®ç±»"""
    
    def __init__(self, 
                 model_path: str = "nano_graphrag/models/modernbert_complexity_classifier",
                 confidence_threshold: float = 0.6,
                 enable_fallback: bool = True,
                 use_modernbert: bool = True):
        self.model_path = model_path
        self.confidence_threshold = confidence_threshold
        self.enable_fallback = enable_fallback
        self.use_modernbert = use_modernbert
    
    def create_router(self) -> ComplexityAwareRouter:
        """åˆ›å»ºå¤æ‚åº¦æ„ŸçŸ¥è·¯ç”±å™¨"""
        return ComplexityAwareRouter(
            model_path=self.model_path,
            confidence_threshold=self.confidence_threshold,
            enable_fallback=self.enable_fallback,
            use_modernbert=self.use_modernbert
        )


#------------------------------------------------------------------------------
# RAGç³»ç»Ÿæ„å»ºå™¨
#------------------------------------------------------------------------------

class RAGSystemBuilder:
    """RAGç³»ç»Ÿæ„å»ºå™¨"""
    
    def __init__(self, working_dir: str, config: ComplexityClassifierConfig):
        self.working_dir = working_dir
        self.config = config
        self.rag = None
    
    def build(self) -> GraphRAG:
        """æ„å»ºRAGç³»ç»Ÿ"""
        print(f"ğŸ”§ æ„å»ºRAGç³»ç»Ÿï¼Œå·¥ä½œç›®å½•: {self.working_dir}")
        
        # åˆ›å»ºGraphRAGå®ä¾‹
        self.rag = GraphRAG(
            working_dir=self.working_dir,
            enable_naive_rag=True,
            enable_bm25=True,
            enable_local=True,
            router_cls=ComplexityAwareRouter,
            router_kwargs={
                "model_path": self.config.model_path,
                "confidence_threshold": self.config.confidence_threshold,
                "enable_fallback": self.config.enable_fallback,
                "use_modernbert": self.config.use_modernbert
            }
        )
        
        return self.rag
    
    def insert_documents(self, documents: List[str]) -> None:
        """æ’å…¥æ–‡æ¡£"""
        if not self.rag:
            raise ValueError("RAGç³»ç»Ÿæœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build()")
        
        print(f"ğŸ“š æ’å…¥ {len(documents)} ä¸ªæ–‡æ¡£")
        
        start_time = time.time()
        for i, doc in enumerate(documents):
            print(f"æ­£åœ¨å¤„ç†æ–‡æ¡£ {i+1}/{len(documents)}...")
            self.rag.insert(doc)
        
        elapsed_time = time.time() - start_time
        print(f"âœ… æ–‡æ¡£æ’å…¥å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")


#------------------------------------------------------------------------------
# æŸ¥è¯¢æµ‹è¯•å™¨
#------------------------------------------------------------------------------

class QueryTester:
    """æŸ¥è¯¢æµ‹è¯•å™¨"""
    
    def __init__(self, rag: GraphRAG):
        self.rag = rag
        self.results = []
    
    async def test_complexity_classification(self, queries: List[str]) -> Dict[str, Any]:
        """æµ‹è¯•å¤æ‚åº¦åˆ†ç±»"""
        print("\nğŸ§  æµ‹è¯•å¤æ‚åº¦åˆ†ç±»å™¨")
        print("="*60)
        
        results = []
        for query in queries:
            complexity_result = await self.rag.router.predict_complexity_detailed(query)
            results.append({
                'query': query,
                'complexity': complexity_result['complexity'],
                'confidence': complexity_result['confidence'],
                'candidate_modes': complexity_result['candidate_modes'],
                'method': complexity_result['method']
            })
            
            print(f"\næŸ¥è¯¢: {query}")
            print(f"é¢„æµ‹å¤æ‚åº¦: {complexity_result['complexity']}")
            print(f"ç½®ä¿¡åº¦: {complexity_result['confidence']:.3f}")
            print(f"å€™é€‰æ¨¡å¼: {complexity_result['candidate_modes']}")
        
        return results
    
    async def test_queries_with_routing(self, queries: List[str]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å¤æ‚åº¦è·¯ç”±æµ‹è¯•æŸ¥è¯¢"""
        print("\nğŸ” æµ‹è¯•å¤æ‚åº¦æ„ŸçŸ¥æŸ¥è¯¢è·¯ç”±")
        print("="*60)
        
        results = []
        for i, query in enumerate(queries, 1):
            print(f"\n--- æŸ¥è¯¢ {i}: {query} ---")
            
            start_time = time.time()
            try:
                response = await self.rag.aquery(query)
                elapsed_time = time.time() - start_time
                
                result = {
                    'query': query,
                    'response': response,
                    'elapsed_time': elapsed_time,
                    'success': True
                }
                results.append(result)
                
                print(f"å›ç­”: {response}")
                print(f"è€—æ—¶: {elapsed_time:.2f}ç§’")
                
            except Exception as e:
                elapsed_time = time.time() - start_time
                result = {
                    'query': query,
                    'response': str(e),
                    'elapsed_time': elapsed_time,
                    'success': False
                }
                results.append(result)
                
                print(f"æŸ¥è¯¢å¤±è´¥: {e}")
        
        return results
    
    def test_different_modes(self, query: str, modes: List[str] = None) -> List[Dict[str, Any]]:
        """æµ‹è¯•ä¸åŒæ£€ç´¢æ¨¡å¼"""
        if modes is None:
            modes = ["llm_only", "naive", "bm25", "local", "global"]
        
        print(f"\nğŸ”„ æµ‹è¯•ä¸åŒæ£€ç´¢æ¨¡å¼: {query}")
        print("="*60)
        
        results = []
        for mode in modes:
            print(f"\n--- æ¨¡å¼: {mode} ---")
            
            start_time = time.time()
            try:
                response = self.rag.query(query, param=QueryParam(mode=mode))
                elapsed_time = time.time() - start_time
                
                result = {
                    'mode': mode,
                    'response': response,
                    'elapsed_time': elapsed_time,
                    'success': True
                }
                results.append(result)
                
                print(f"å›ç­”: {response}")
                print(f"è€—æ—¶: {elapsed_time:.2f}ç§’")
                
            except Exception as e:
                elapsed_time = time.time() - start_time
                result = {
                    'mode': mode,
                    'response': str(e),
                    'elapsed_time': elapsed_time,
                    'success': False
                }
                results.append(result)
                
                print(f"æŸ¥è¯¢å¤±è´¥: {e}")
        
        return results


#------------------------------------------------------------------------------
# æ€§èƒ½åˆ†æå™¨
#------------------------------------------------------------------------------

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, rag: GraphRAG, output_dir: str):
        self.rag = rag
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
    
    def analyze_system_performance(self) -> Dict[str, Any]:
        """åˆ†æç³»ç»Ÿæ€§èƒ½"""
        print("\nğŸ“Š ç³»ç»Ÿæ€§èƒ½åˆ†æ")
        print("="*60)
        
        # è·å–å¤æ‚åº¦ç»Ÿè®¡
        complexity_stats = {}
        if hasattr(self.rag.router, 'get_complexity_stats'):
            complexity_stats = self.rag.router.get_complexity_stats()
            print(f"å¤æ‚åº¦åˆ†ç±»ç»Ÿè®¡: {complexity_stats}")
        
        # æ£€æŸ¥å­˜å‚¨çŠ¶æ€
        storage_stats = self._analyze_storage()
        
        # ç³»ç»Ÿä¿¡æ¯
        system_info = {
            'working_dir': self.rag.working_dir,
            'complexity_stats': complexity_stats,
            'storage_stats': storage_stats,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = os.path.join(self.output_dir, 'performance_analysis.json')
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(system_info, f, ensure_ascii=False, indent=2)
        
        print(f"æ€§èƒ½åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_file}")
        return system_info
    
    def _analyze_storage(self) -> Dict[str, Any]:
        """åˆ†æå­˜å‚¨çŠ¶æ€"""
        working_dir = self.rag.working_dir
        storage_stats = {}
        
        files_to_check = [
            "vdb_chunks.json",
            "vdb_entities.json", 
            "kv_store_text_chunks.json",
            "kv_store_entities.json",
            "kv_store_community_reports.json",
            "graph_chunk_entity_relation.graphml"
        ]
        
        print(f"\nå­˜å‚¨çŠ¶æ€:")
        print(f"å·¥ä½œç›®å½•: {working_dir}")
        
        total_size = 0
        for file_name in files_to_check:
            file_path = os.path.join(working_dir, file_name)
            exists = os.path.exists(file_path)
            size = os.path.getsize(file_path) if exists else 0
            total_size += size
            
            status = 'âœ…' if exists else 'âŒ'
            print(f"  {file_name}: {status} ({size} bytes)")
            
            storage_stats[file_name] = {
                'exists': exists,
                'size': size
            }
        
        storage_stats['total_size'] = total_size
        print(f"æ€»å­˜å‚¨å¤§å°: {total_size} bytes")
        
        return storage_stats
    
    def save_test_results(self, results: List[Dict[str, Any]], filename: str) -> None:
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        results_file = os.path.join(self.output_dir, filename)
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")


#------------------------------------------------------------------------------
# ä¸»å‡½æ•°
#------------------------------------------------------------------------------

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é«˜çº§å¤æ‚åº¦åˆ†ç±»å™¨RAGæµç¨‹æ¼”ç¤º')
    parser.add_argument('--docs_path', type=str, default='./sample_docs',
                       help='æ–‡æ¡£è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰')
    parser.add_argument('--output_dir', type=str, default='./results',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--working_dir', type=str, default='./complexity_classifier_cache',
                       help='å·¥ä½œç›®å½•')
    parser.add_argument('--confidence_threshold', type=float, default=0.6,
                       help='å¤æ‚åº¦åˆ†ç±»ç½®ä¿¡åº¦é˜ˆå€¼')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹é«˜çº§å¤æ‚åº¦åˆ†ç±»å™¨RAGæµç¨‹æ¼”ç¤º")
    print("="*60)
    
    # 1. åŠ è½½æ–‡æ¡£
    print(f"\nğŸ“ æ­¥éª¤1: ä» {args.docs_path} åŠ è½½æ–‡æ¡£")
    documents = DocumentLoader.load_documents(args.docs_path)
    print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    if not documents:
        print("âš ï¸ æ²¡æœ‰åŠ è½½åˆ°æ–‡æ¡£ï¼Œä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£")
        # ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£
        sample_docs = [
            "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
            "æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚",
            "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚"
        ]
        documents = sample_docs
    
    # 2. é…ç½®å¤æ‚åº¦åˆ†ç±»å™¨
    print(f"\nâš™ï¸ æ­¥éª¤2: é…ç½®å¤æ‚åº¦åˆ†ç±»å™¨")
    config = ComplexityClassifierConfig(
        confidence_threshold=args.confidence_threshold
    )
    
    # 3. æ„å»ºRAGç³»ç»Ÿ
    print(f"\nğŸ”§ æ­¥éª¤3: æ„å»ºRAGç³»ç»Ÿ")
    builder = RAGSystemBuilder(args.working_dir, config)
    rag = builder.build()
    
    # 4. æ’å…¥æ–‡æ¡£
    print(f"\nğŸ“š æ­¥éª¤4: æ’å…¥æ–‡æ¡£")
    builder.insert_documents(documents)
    
    # 5. æµ‹è¯•æŸ¥è¯¢
    print(f"\nğŸ” æ­¥éª¤5: æµ‹è¯•æŸ¥è¯¢")
    tester = QueryTester(rag)
    
    # æµ‹è¯•æŸ¥è¯¢é›†
    test_queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç®—æ³•ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    # æµ‹è¯•å¤æ‚åº¦åˆ†ç±»
    complexity_results = await tester.test_complexity_classification(test_queries)
    
    # æµ‹è¯•å¤æ‚åº¦è·¯ç”±æŸ¥è¯¢
    routing_results = await tester.test_queries_with_routing(test_queries)
    
    # æµ‹è¯•ä¸åŒæ¨¡å¼
    mode_results = tester.test_different_modes("æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ")
    
    # 6. æ€§èƒ½åˆ†æ
    print(f"\nğŸ“Š æ­¥éª¤6: æ€§èƒ½åˆ†æ")
    analyzer = PerformanceAnalyzer(rag, args.output_dir)
    performance_results = analyzer.analyze_system_performance()
    
    # ä¿å­˜ç»“æœ
    analyzer.save_test_results(complexity_results, 'complexity_classification_results.json')
    analyzer.save_test_results(routing_results, 'routing_test_results.json')
    analyzer.save_test_results(mode_results, 'mode_comparison_results.json')
    
    print("\n" + "="*60)
    print("ğŸ‰ é«˜çº§å¤æ‚åº¦åˆ†ç±»å™¨RAGæµç¨‹æ¼”ç¤ºå®Œæˆï¼")
    print("="*60)
    
    # è¾“å‡ºæ‘˜è¦
    print(f"\nğŸ“‹ ç»“æœæ‘˜è¦:")
    print(f"- å¤„ç†æ–‡æ¡£æ•°: {len(documents)}")
    print(f"- æµ‹è¯•æŸ¥è¯¢æ•°: {len(test_queries)}")
    print(f"- å¤æ‚åº¦åˆ†ç±»å‡†ç¡®ç‡: {sum(1 for r in complexity_results if r['confidence'] > 0.7) / len(complexity_results):.1%}")
    print(f"- æŸ¥è¯¢æˆåŠŸç‡: {sum(1 for r in routing_results if r['success']) / len(routing_results):.1%}")
    print(f"- å¹³å‡æŸ¥è¯¢æ—¶é—´: {sum(r['elapsed_time'] for r in routing_results) / len(routing_results):.2f}ç§’")
    print(f"- ç»“æœä¿å­˜ä½ç½®: {args.output_dir}")


if __name__ == "__main__":
    asyncio.run(main()) 