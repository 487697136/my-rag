"""
ä½¿ç”¨å¤æ‚åº¦åˆ†ç±»å™¨çš„å®Œæ•´RAGæµç¨‹ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨nano-graphragçš„å¤æ‚åº¦åˆ†ç±»å™¨è¿›è¡Œå®Œæ•´çš„RAGæµç¨‹ï¼š
1. è¯­æ–™åº“å‡†å¤‡å’Œæ–‡æ¡£æ’å…¥
2. çŸ¥è¯†å›¾è°±å’Œå‘é‡æ•°æ®åº“æ„å»º
3. å¤æ‚åº¦åˆ†ç±»å™¨é…ç½®
4. è‡ªé€‚åº”æŸ¥è¯¢è·¯ç”±å’Œå›ç­”ç”Ÿæˆ

åŒ…æ‹¬ï¼š
- æ–‡æ¡£é¢„å¤„ç†å’Œåˆ†å—
- å®ä½“æå–å’Œå…³ç³»æ„å»º
- å‘é‡åŒ–å’Œå­˜å‚¨
- å¤æ‚åº¦æ„ŸçŸ¥çš„æŸ¥è¯¢è·¯ç”±
- å¤šæ¨¡å¼æ£€ç´¢å’Œå›ç­”ç”Ÿæˆ
"""

import os
import sys
import json
import asyncio
import logging
import httpx
import numpy as np
from typing import List, Dict, Any, Optional
from pathlib import Path
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from openai import OpenAI

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from nano_graphrag import GraphRAG, QueryParam
from nano_graphrag.complexity import ComplexityAwareRouter, ComplexityClassifier
from nano_graphrag._utils import logger, compute_args_hash, wrap_embedding_func_with_attrs
from nano_graphrag.base import BaseKVStorage, BaseGraphStorage, BaseVectorStorage

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.WARNING)
logging.getLogger("nano-graphrag").setLevel(logging.INFO)

# APIå¯†é’¥å’ŒåŸºç¡€URLé…ç½®
# ç¡…åŸºæµåŠ¨API
SILKFLOW_API_KEY = "sk-rwcxtompyeenjkhdpuganvhsfmmctoftyfcqwpsgtchochkv"
SILKFLOW_API_BASE = "https://api.siliconflow.cn/v1"

# é˜¿é‡Œäº‘ç™¾ç‚¼API
DASHSCOPE_API_KEY = os.environ.get("DASHSCOPE_API_KEY", "YOUR_DASHSCOPE_API_KEY")
DASHSCOPE_API_BASE = "https://dashscope.aliyuncs.com/compatible-mode/v1"

# æ¨¡å‹é…ç½®
SILKFLOW_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ç¡…åŸºæµåŠ¨æ¨¡å‹
DASHSCOPE_MODEL = "qwen-turbo"               # é˜¿é‡Œäº‘ç™¾ç‚¼æ¨¡å‹
EMBED_MODEL = "BAAI/bge-m3"                  # åµŒå…¥æ¨¡å‹ï¼ˆ1024ç»´ï¼‰

# å·¥ä½œç›®å½•é…ç½®
WORKING_DIR = "./complexity_classifier_cache"

# ç¤ºä¾‹æ–‡æ¡£æ•°æ®
SAMPLE_DOCUMENTS = [
    {
        "title": "äººå·¥æ™ºèƒ½åŸºç¡€",
        "content": """
äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
AIçš„ä¸»è¦åˆ†æ”¯åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚

æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯AIçš„å¦ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
NLPçš„åº”ç”¨åŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æã€é—®ç­”ç³»ç»Ÿç­‰ã€‚

å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚
å®ƒåœ¨æ¸¸æˆã€æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚
        """
    },
    {
        "title": "æœºå™¨å­¦ä¹ ç®—æ³•",
        "content": """
æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥åˆ†ä¸ºä¸‰å¤§ç±»ï¼šç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚

ç›‘ç£å­¦ä¹ ç®—æ³•åŒ…æ‹¬ï¼š
- çº¿æ€§å›å½’ï¼šç”¨äºé¢„æµ‹è¿ç»­å€¼
- é€»è¾‘å›å½’ï¼šç”¨äºåˆ†ç±»é—®é¢˜
- å†³ç­–æ ‘ï¼šåŸºäºç‰¹å¾è¿›è¡Œå†³ç­–
- éšæœºæ£®æ—ï¼šé›†æˆå¤šä¸ªå†³ç­–æ ‘
- æ”¯æŒå‘é‡æœºï¼šå¯»æ‰¾æœ€ä¼˜åˆ†ç±»è¾¹ç•Œ
- ç¥ç»ç½‘ç»œï¼šæ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒ

æ— ç›‘ç£å­¦ä¹ ç®—æ³•åŒ…æ‹¬ï¼š
- K-meansèšç±»ï¼šå°†æ•°æ®åˆ†ç»„
- ä¸»æˆåˆ†åˆ†æï¼šé™ç»´æŠ€æœ¯
- è‡ªç¼–ç å™¨ï¼šå­¦ä¹ æ•°æ®è¡¨ç¤º
- ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼šç”Ÿæˆæ–°æ•°æ®

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œï¼š
- å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼šç”¨äºå›¾åƒå¤„ç†
- å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼šç”¨äºåºåˆ—æ•°æ®
- é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ï¼šæ”¹è¿›çš„RNN
- Transformerï¼šç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†
        """
    },
    {
        "title": "æ·±åº¦å­¦ä¹ æ¡†æ¶",
        "content": """
ä¸»æµçš„æ·±åº¦å­¦ä¹ æ¡†æ¶åŒ…æ‹¬ï¼š

PyTorchæ˜¯ç”±Facebookå¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå…·æœ‰åŠ¨æ€è®¡ç®—å›¾çš„ç‰¹æ€§ã€‚
å®ƒæä¾›äº†çµæ´»çš„ç¼–ç¨‹æ¥å£ï¼Œæ”¯æŒå¿«é€ŸåŸå‹å¼€å‘å’Œå®éªŒã€‚
PyTorchåœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéƒ½å¹¿æ³›ä½¿ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç ”ç©¶é¢†åŸŸã€‚

TensorFlowæ˜¯ç”±Googleå¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå…·æœ‰é™æ€è®¡ç®—å›¾çš„ç‰¹æ€§ã€‚
å®ƒæä¾›äº†å¼ºå¤§çš„ç”Ÿäº§ç¯å¢ƒæ”¯æŒï¼ŒåŒ…æ‹¬TensorFlow Servingå’ŒTensorFlow Liteã€‚
TensorFlowåœ¨å·¥ä¸šéƒ¨ç½²æ–¹é¢æœ‰ä¼˜åŠ¿ã€‚

Kerasæ˜¯ä¸€ä¸ªé«˜çº§ç¥ç»ç½‘ç»œAPIï¼Œå¯ä»¥è¿è¡Œåœ¨TensorFlowã€Theanoæˆ–CNTKä¹‹ä¸Šã€‚
å®ƒæä¾›äº†ç®€å•æ˜“ç”¨çš„æ¥å£ï¼Œé€‚åˆåˆå­¦è€…ä½¿ç”¨ã€‚

JAXæ˜¯Googleå¼€å‘çš„ç”¨äºé«˜æ€§èƒ½æœºå™¨å­¦ä¹ ç ”ç©¶çš„æ¡†æ¶ã€‚
å®ƒç»“åˆäº†NumPyçš„æ˜“ç”¨æ€§å’ŒGPU/TPUçš„åŠ é€Ÿèƒ½åŠ›ã€‚

è¿™äº›æ¡†æ¶éƒ½æ”¯æŒè‡ªåŠ¨å¾®åˆ†ã€GPUåŠ é€Ÿå’Œåˆ†å¸ƒå¼è®­ç»ƒç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚
é€‰æ‹©æ¡†æ¶æ—¶éœ€è¦è€ƒè™‘é¡¹ç›®éœ€æ±‚ã€å›¢é˜ŸæŠ€èƒ½å’Œéƒ¨ç½²ç¯å¢ƒç­‰å› ç´ ã€‚
        """
    }
]

# æµ‹è¯•æŸ¥è¯¢é›†
TEST_QUERIES = [
    # Zero-hopæŸ¥è¯¢ï¼ˆå¸¸è¯†æ€§é—®é¢˜ï¼‰
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "2+2ç­‰äºå¤šå°‘ï¼Ÿ",
    "æ°´çš„æ²¸ç‚¹æ˜¯å¤šå°‘ï¼Ÿ",
    
    # One-hopæŸ¥è¯¢ï¼ˆå•æ­¥æ£€ç´¢ï¼‰
    "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ä¸»è¦ç®—æ³•ï¼Ÿ",
    "PyTorchæ˜¯ä»€ä¹ˆï¼Ÿ",
    "æ·±åº¦å­¦ä¹ æ¡†æ¶æœ‰å“ªäº›ï¼Ÿ",
    
    # Multi-hopæŸ¥è¯¢ï¼ˆå¤šæ­¥æ¨ç†ï¼‰
    "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ",
    "PyTorchç›¸æ¯”TensorFlowæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
    "åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­ï¼Œä¸ºä»€ä¹ˆCNNæ¯”RNNæ›´æœ‰æ•ˆï¼Ÿ",
    "å¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ"
]


#------------------------------------------------------------------------------
# APIå‡½æ•°
#------------------------------------------------------------------------------

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type((httpx.ReadTimeout, httpx.ConnectTimeout, httpx.HTTPStatusError))
)
async def silkflow_llm_api(
    prompt: str, 
    system_prompt: Optional[str] = None, 
    history_messages: List[Dict[str, str]] = [], 
    **kwargs
) -> str:
    """è°ƒç”¨ç¡…åŸºæµåŠ¨LLM APIçš„å‡½æ•°"""
    # å‡†å¤‡æ¶ˆæ¯æ ¼å¼
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    # ç¼“å­˜å¤„ç†
    hashing_kv: BaseKVStorage = kwargs.pop("hashing_kv", None)
    messages.extend(history_messages)
    messages.append({"role": "user", "content": prompt})
    
    if hashing_kv is not None:
        args_hash = compute_args_hash(SILKFLOW_MODEL, messages)
        if_cache_return = await hashing_kv.get_by_id(args_hash)
        if if_cache_return is not None:
            return if_cache_return["return"]
    
    # å‡†å¤‡è¯·æ±‚å¤´å’Œè¯·æ±‚ä½“
    headers = {
        "Authorization": f"Bearer {SILKFLOW_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # ä»kwargsæå–ç›¸å…³å‚æ•°
    max_tokens = kwargs.pop("max_tokens", 512)
    temperature = kwargs.pop("temperature", 0.7)
    
    # è¯·æ±‚ä½“
    payload = {
        "model": SILKFLOW_MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    
    # å¯é€‰å‚æ•°
    if kwargs.get("stream", False):
        payload["stream"] = kwargs["stream"]
    
    try:
        # å‘é€è¯·æ±‚
        async with httpx.AsyncClient(timeout=180.0) as client:
            logger.info(f"æ­£åœ¨å‘é€è¯·æ±‚åˆ° {SILKFLOW_API_BASE}/chat/completions")
            logger.debug(f"è¯·æ±‚å¤´: {headers}")
            logger.debug(f"è¯·æ±‚ä½“: {payload}")
            
            response = await client.post(
                f"{SILKFLOW_API_BASE}/chat/completions",
                headers=headers,
                json=payload
            )
            
            # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œè®°å½•é”™è¯¯è¯¦æƒ…
            if response.status_code != 200:
                logger.error(f"APIè¯·æ±‚å¤±è´¥ ({response.status_code}): {response.text}")
                response.raise_for_status()
                
            result = response.json()
            logger.debug(f"APIå“åº”: {result}")
        
        # æå–å“åº”å†…å®¹
        content = result["choices"][0]["message"]["content"]
        
        # ç¼“å­˜å“åº”
        if hashing_kv is not None:
            await hashing_kv.upsert(
                {args_hash: {"return": content, "model": SILKFLOW_MODEL}}
            )
        
        return content
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 401:
            logger.error(f"APIè®¤è¯å¤±è´¥ (HTTP 401)ï¼šè¯·æ£€æŸ¥æ‚¨çš„APIå¯†é’¥æ˜¯å¦æ­£ç¡®")
        elif e.response.status_code == 400:
            logger.error(f"APIè¯·æ±‚æ ¼å¼é”™è¯¯ (HTTP 400)ï¼š{e.response.text}")
        elif e.response.status_code == 429:
            retry_after = int(e.response.headers.get("Retry-After", 10))
            await asyncio.sleep(retry_after)      # ç­‰å¾…å†é‡è¯•
            raise
        else:
            logger.error(f"APIè¯·æ±‚å¤±è´¥ ({e.response.status_code}): {e.response.text}")
        raise
    except Exception as e:
        logger.error(f"APIè¯·æ±‚è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise


@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type((httpx.ReadTimeout, httpx.ConnectTimeout, httpx.HTTPStatusError))
)
async def dashscope_llm_api(
    prompt: str, 
    system_prompt: Optional[str] = None, 
    history_messages: List[Dict[str, str]] = [], 
    **kwargs
) -> str:
    """è°ƒç”¨é˜¿é‡Œäº‘ç™¾ç‚¼LLM APIçš„å‡½æ•°"""
    # å‡†å¤‡æ¶ˆæ¯æ ¼å¼
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    # ç¼“å­˜å¤„ç†
    hashing_kv: BaseKVStorage = kwargs.pop("hashing_kv", None)
    messages.extend(history_messages)
    messages.append({"role": "user", "content": prompt})
    
    if hashing_kv is not None:
        args_hash = compute_args_hash(DASHSCOPE_MODEL, messages)
        if_cache_return = await hashing_kv.get_by_id(args_hash)
        if if_cache_return is not None:
            return if_cache_return["return"]
    
    try:
        # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨é˜¿é‡Œäº‘ç™¾ç‚¼API
        client = OpenAI(
            api_key=DASHSCOPE_API_KEY,
            base_url=DASHSCOPE_API_BASE,
        )
        
        # ä»kwargsæå–ç›¸å…³å‚æ•°
        max_tokens = kwargs.pop("max_tokens", 512)
        temperature = kwargs.pop("temperature", 0.7)
        
        # åˆ›å»ºèŠå¤©å®Œæˆè¯·æ±‚
        completion = client.chat.completions.create(
            model=DASHSCOPE_MODEL,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            **kwargs
        )
        
        # æå–å“åº”å†…å®¹
        content = completion.choices[0].message.content
        
        # ç¼“å­˜å“åº”
        if hashing_kv is not None:
            await hashing_kv.upsert(
                {args_hash: {"return": content, "model": DASHSCOPE_MODEL}}
            )
        
        return content
        
    except Exception as e:
        logger.error(f"é˜¿é‡Œäº‘ç™¾ç‚¼APIè¯·æ±‚è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise


async def simple_entity_extraction(
    inserting_chunks: Dict[str, Dict],
    knwoledge_graph_inst: BaseGraphStorage,
    entity_vdb: Optional[BaseVectorStorage],
    global_config: Dict,
    using_amazon_bedrock: bool = False,
) -> Optional[BaseGraphStorage]:
    """ç®€åŒ–çš„å®ä½“æå–å‡½æ•°ï¼Œé¿å…ä½¿ç”¨æœ‰é—®é¢˜çš„pack_user_ass_to_openai_messages"""
    # ç®€å•è¿”å›ç°æœ‰çš„çŸ¥è¯†å›¾è°±ï¼Œä¸è¿›è¡Œå®ä½“æå–
    logger.info("ä½¿ç”¨ç®€åŒ–çš„å®ä½“æå–å‡½æ•°ï¼Œè·³è¿‡å®ä½“æå–æ­¥éª¤")
    return knwoledge_graph_inst


async def simple_community_report(
    community_reports: BaseKVStorage,
    chunk_entity_relation_graph: BaseGraphStorage,
    global_config: Dict
) -> None:
    """ç®€åŒ–çš„ç¤¾åŒºæŠ¥å‘Šç”Ÿæˆå‡½æ•°ï¼Œè·³è¿‡èšç±»æ­¥éª¤"""
    logger.info("ä½¿ç”¨ç®€åŒ–çš„ç¤¾åŒºæŠ¥å‘Šç”Ÿæˆå‡½æ•°ï¼Œè·³è¿‡èšç±»æ­¥éª¤")
    # ä¸è¿›è¡Œä»»ä½•æ“ä½œï¼Œç›´æ¥è¿”å›
    pass


@wrap_embedding_func_with_attrs(embedding_dim=1024, max_token_size=4096)  # bge-m3æ˜¯1024ç»´
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type((httpx.ReadTimeout, httpx.ConnectTimeout, httpx.HTTPStatusError))
)
async def silkflow_embedding(texts: List[str]) -> np.ndarray:
    """è°ƒç”¨ç¡…åŸºæµåŠ¨åµŒå…¥APIçš„å‡½æ•°"""
    
    # ä½¿ç”¨ç¡…åŸºæµåŠ¨APIå¯†é’¥
    headers = {
        "Authorization": f"Bearer {SILKFLOW_API_KEY}",
        "Content-Type": "application/json"
    }
    
    all_embeddings = []
    
    # å°è¯•æ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡æœ¬ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªä¸€ä¸ªå¤„ç†
    try:
        payload = {
            "model": EMBED_MODEL,
            "input": texts,
            "encoding_format": "float"
        }
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            logger.info(f"æ­£åœ¨å‘é€åµŒå…¥è¯·æ±‚åˆ° {SILKFLOW_API_BASE}/embeddings")
            logger.debug(f"è¯·æ±‚ä½“: {payload}")
            
            response = await client.post(
                f"{SILKFLOW_API_BASE}/embeddings",
                headers=headers,
                json=payload
            )
            
            # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œè®°å½•é”™è¯¯è¯¦æƒ…
            if response.status_code != 200:
                logger.error(f"åµŒå…¥APIè¯·æ±‚å¤±è´¥ ({response.status_code}): {response.text}")
                response.raise_for_status()
                
            result = response.json()
            logger.debug(f"åµŒå…¥APIå“åº”: {result}")
            
            # è·å–æ‰€æœ‰åµŒå…¥å‘é‡
            all_embeddings = [item["embedding"] for item in result["data"]]
            
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 401:
            logger.error(f"åµŒå…¥APIè®¤è¯å¤±è´¥ (HTTP 401)ï¼šè¯·æ£€æŸ¥æ‚¨çš„APIå¯†é’¥æ˜¯å¦æ­£ç¡®")
        elif e.response.status_code == 400:
            logger.error(f"åµŒå…¥APIè¯·æ±‚æ ¼å¼é”™è¯¯ (HTTP 400)ï¼š{e.response.text}")
            
            # å¦‚æœæ‰¹é‡è¯·æ±‚å¤±è´¥ï¼Œå°è¯•é€ä¸ªè¯·æ±‚
            logger.info("å°è¯•é€ä¸ªå‘é€åµŒå…¥è¯·æ±‚...")
            for text in texts:
                try:
                    payload = {
                        "model": EMBED_MODEL,
                        "input": text,
                        "encoding_format": "float"
                    }
                    
                    async with httpx.AsyncClient(timeout=60.0) as client:
                        response = await client.post(
                            f"{SILKFLOW_API_BASE}/embeddings",
                            headers=headers,
                            json=payload
                        )
                        response.raise_for_status()
                        result = response.json()
                        embedding = result["data"][0]["embedding"]
                        all_embeddings.append(embedding)
                except Exception as e:
                    logger.error(f"å•ä¸ªåµŒå…¥è¯·æ±‚å¤±è´¥: {str(e)}")
                    # æ·»åŠ é›¶å‘é‡ä½œä¸ºæ›¿ä»£
                    all_embeddings.append([0.0] * 1024)
        else:
            logger.error(f"åµŒå…¥APIè¯·æ±‚å¤±è´¥ ({e.response.status_code}): {e.response.text}")
        if not all_embeddings:
            raise
    except Exception as e:
        logger.error(f"åµŒå…¥APIè¯·æ±‚è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise
    
    return np.array(all_embeddings)


#------------------------------------------------------------------------------
# æ–‡æ¡£é¢„å¤„ç†å‡½æ•°
#------------------------------------------------------------------------------

def prepare_documents() -> List[str]:
    """å‡†å¤‡æ–‡æ¡£æ•°æ®"""
    documents = []
    for doc in SAMPLE_DOCUMENTS:
        # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹
        full_content = f"æ ‡é¢˜ï¼š{doc['title']}\n\nå†…å®¹ï¼š{doc['content']}"
        documents.append(full_content)
    return documents


def save_documents_to_file(documents: List[str], file_path: str) -> None:
    """å°†æ–‡æ¡£ä¿å­˜åˆ°æ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        for i, doc in enumerate(documents):
            f.write(f"=== æ–‡æ¡£ {i+1} ===\n")
            f.write(doc)
            f.write("\n\n")


#------------------------------------------------------------------------------
# å¤æ‚åº¦åˆ†ç±»å™¨é…ç½®
#------------------------------------------------------------------------------

def create_complexity_router() -> ComplexityAwareRouter:
    """åˆ›å»ºå¤æ‚åº¦æ„ŸçŸ¥è·¯ç”±å™¨"""
    router = ComplexityAwareRouter(
        model_path="nano_graphrag/models/modernbert_complexity_classifier",
        confidence_threshold=0.6,
        enable_fallback=True,
        use_modernbert=True
    )
    return router


async def test_complexity_classification(router: ComplexityAwareRouter) -> None:
    """æµ‹è¯•å¤æ‚åº¦åˆ†ç±»åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ§  æµ‹è¯•å¤æ‚åº¦åˆ†ç±»å™¨")
    print("="*60)
    
    for query in TEST_QUERIES:
        # é¢„æµ‹å¤æ‚åº¦
        complexity_result = await router.predict_complexity_detailed(query)
        
        print(f"\næŸ¥è¯¢: {query}")
        print(f"é¢„æµ‹å¤æ‚åº¦: {complexity_result['complexity']}")
        print(f"ç½®ä¿¡åº¦: {complexity_result['confidence']:.3f}")
        print(f"å€™é€‰æ¨¡å¼: {complexity_result['candidate_modes']}")
        print(f"åˆ†ç±»æ–¹æ³•: {complexity_result['method']}")


#------------------------------------------------------------------------------
# RAGç³»ç»Ÿæ„å»º
#------------------------------------------------------------------------------

def build_rag_system(working_dir: str) -> GraphRAG:
    """æ„å»ºRAGç³»ç»Ÿ"""
    print(f"\nğŸ”§ æ„å»ºRAGç³»ç»Ÿï¼Œå·¥ä½œç›®å½•: {working_dir}")
    
    # åˆ›å»ºGraphRAGå®ä¾‹ï¼Œå¯ç”¨å¤æ‚åº¦æ„ŸçŸ¥è·¯ç”±
    rag = GraphRAG(
        working_dir=working_dir,
        enable_naive_rag=True,
        enable_bm25=True,
        enable_local=False,  # ç¦ç”¨localæ¨¡å¼ä»¥é¿å…å®ä½“æå–é—®é¢˜
        # ä½¿ç”¨APIæ¨¡å‹
        best_model_func=dashscope_llm_api,  # ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼API
        cheap_model_func=dashscope_llm_api,  # ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼API
        embedding_func=silkflow_embedding,   # ä½¿ç”¨ç¡…åŸºæµåŠ¨åµŒå…¥API
        # é…ç½®å‘é‡æ•°æ®åº“ä½¿ç”¨1024ç»´
        vector_db_storage_cls_kwargs={
            "embedding_dim": 1024
        },
        # ä½¿ç”¨å¤æ‚åº¦æ„ŸçŸ¥è·¯ç”±å™¨
        router_cls=ComplexityAwareRouter,
        router_kwargs={
            "model_path": "nano_graphrag/models/modernbert_complexity_classifier",
            "confidence_threshold": 0.6,
            "enable_fallback": True,
            "use_modernbert": True
        }
    )
    
    # æ›¿æ¢å®ä½“æå–å‡½æ•°
    rag.entity_extraction_func = simple_entity_extraction
    
    # æ›¿æ¢ç¤¾åŒºæŠ¥å‘Šç”Ÿæˆå‡½æ•°
    # è¿™é‡Œæˆ‘ä»¬å°†åœ¨æ’å…¥æ–‡æ¡£æ—¶æ‰‹åŠ¨å¤„ç†ï¼Œé¿å…è°ƒç”¨æœ‰é—®é¢˜çš„èšç±»å‡½æ•°
    
    return rag


async def insert_documents(rag: GraphRAG, documents: List[str]) -> None:
    """æ’å…¥æ–‡æ¡£åˆ°RAGç³»ç»Ÿ"""
    print(f"\nğŸ“š æ’å…¥ {len(documents)} ä¸ªæ–‡æ¡£")
    
    for i, doc in enumerate(documents):
        print(f"æ­£åœ¨å¤„ç†æ–‡æ¡£ {i+1}/{len(documents)}...")
        try:
            # æ‰‹åŠ¨å¤„ç†æ–‡æ¡£æ’å…¥ï¼Œé¿å…èšç±»é—®é¢˜
            await manual_insert_document(rag, doc)
        except Exception as e:
            print(f"æ–‡æ¡£ {i+1} æ’å…¥å¤±è´¥: {e}")
            # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡æ¡£
            continue
    
    print("âœ… æ–‡æ¡£æ’å…¥å®Œæˆ")


async def manual_insert_document(rag: GraphRAG, document: str) -> None:
    """æ‰‹åŠ¨æ’å…¥æ–‡æ¡£ï¼Œé¿å…èšç±»é—®é¢˜"""
    # ç›´æ¥è°ƒç”¨GraphRAGçš„ainsertæ–¹æ³•ï¼Œä½†åœ¨èšç±»æ­¥éª¤å‰åœæ­¢
    await rag._insert_start()
    
    try:
        # å¤„ç†æ–‡æ¡£
        if isinstance(document, str):
            documents = [document]
        
        # åˆ›å»ºæ–°æ–‡æ¡£
        from nano_graphrag._utils import compute_mdhash_id
        new_docs = {
            compute_mdhash_id(c.strip(), prefix="doc-"): {"content": c.strip()}
            for c in documents
        }
        
        # è¿‡æ»¤å·²å­˜åœ¨çš„æ–‡æ¡£
        _add_doc_keys = await rag.full_docs.filter_keys(list(new_docs.keys()))
        new_docs = {k: v for k, v in new_docs.items() if k in _add_doc_keys}
        
        if not len(new_docs):
            logger.warning(f"æ‰€æœ‰æ–‡æ¡£éƒ½å·²å­˜åœ¨äºå­˜å‚¨ä¸­")
            return
        
        logger.info(f"[New Docs] inserting {len(new_docs)} docs")
        
        # åˆ†å—å¤„ç†
        from nano_graphrag._op import get_chunks
        inserting_chunks = get_chunks(
            new_docs=new_docs,
            chunk_func=rag.chunk_func,
            overlap_token_size=rag.chunk_overlap_token_size,
            max_token_size=rag.chunk_token_size,
        )
        
        _add_chunk_keys = await rag.text_chunks.filter_keys(list(inserting_chunks.keys()))
        inserting_chunks = {k: v for k, v in inserting_chunks.items() if k in _add_chunk_keys}
        
        if not len(inserting_chunks):
            logger.warning(f"æ‰€æœ‰å—éƒ½å·²å­˜åœ¨äºå­˜å‚¨ä¸­")
            return
        
        logger.info(f"[New Chunks] inserting {len(inserting_chunks)} chunks")
        
        # æ’å…¥åˆ°å‘é‡æ•°æ®åº“
        if rag.enable_naive_rag:
            logger.info("Insert chunks for naive RAG")
            await rag.chunks_vdb.upsert(inserting_chunks)
        
        # ä¸ºBM25ç´¢å¼•æ–‡æ¡£
        if rag.enable_bm25:
            logger.info("Indexing documents for BM25")
            bm25_docs = {k: v["content"] for k, v in inserting_chunks.items()}
            await rag.bm25_store.index_documents(bm25_docs)
        
        # æ¸…ç©ºç¤¾åŒºæŠ¥å‘Š
        await rag.community_reports.drop()
        
        # ä½¿ç”¨ç®€åŒ–çš„å®ä½“æå–
        logger.info("[Entity Extraction]...")
        maybe_new_kg = await rag.entity_extraction_func(
            inserting_chunks,
            rag.chunk_entity_relation_graph,
            rag.entities_vdb,
            rag.__dict__,
            rag.using_amazon_bedrock,
        )
        
        if maybe_new_kg is not None:
            rag.chunk_entity_relation_graph = maybe_new_kg
        
        # è·³è¿‡èšç±»æ­¥éª¤ï¼Œç›´æ¥æäº¤
        logger.info("[Community Report]...")
        logger.info("è·³è¿‡èšç±»æ­¥éª¤ä»¥é¿å…ç©ºå›¾é—®é¢˜")
        
        # æäº¤æ‰€æœ‰æ›´æ”¹
        await rag.full_docs.upsert(new_docs)
        await rag.text_chunks.upsert(inserting_chunks)
        
    finally:
        await rag._insert_done()


#------------------------------------------------------------------------------
# æŸ¥è¯¢æµ‹è¯•
#------------------------------------------------------------------------------

async def test_queries_with_complexity_routing(rag: GraphRAG) -> None:
    """ä½¿ç”¨å¤æ‚åº¦è·¯ç”±æµ‹è¯•æŸ¥è¯¢"""
    print("\n" + "="*60)
    print("ğŸ” æµ‹è¯•å¤æ‚åº¦æ„ŸçŸ¥æŸ¥è¯¢è·¯ç”±")
    print("="*60)
    
    for i, query in enumerate(TEST_QUERIES, 1):
        print(f"\n--- æŸ¥è¯¢ {i}: {query} ---")
        
        try:
            # ä½¿ç”¨å¤æ‚åº¦æ„ŸçŸ¥è·¯ç”±è¿›è¡ŒæŸ¥è¯¢
            response = await rag.aquery(query)
            
            print(f"å›ç­”: {response}")
            
        except Exception as e:
            print(f"æŸ¥è¯¢å¤±è´¥: {e}")


def test_different_modes(rag: GraphRAG) -> None:
    """æµ‹è¯•ä¸åŒæ£€ç´¢æ¨¡å¼"""
    print("\n" + "="*60)
    print("ğŸ”„ æµ‹è¯•ä¸åŒæ£€ç´¢æ¨¡å¼")
    print("="*60)
    
    test_query = "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ"
    
    modes = ["llm_only", "naive", "bm25", "local", "global"]
    
    for mode in modes:
        print(f"\n--- æ¨¡å¼: {mode} ---")
        try:
            response = rag.query(test_query, param=QueryParam(mode=mode))
            print(f"å›ç­”: {response}")
        except Exception as e:
            print(f"æŸ¥è¯¢å¤±è´¥: {e}")


#------------------------------------------------------------------------------
# æ€§èƒ½åˆ†æ
#------------------------------------------------------------------------------

def analyze_system_performance(rag: GraphRAG) -> None:
    """åˆ†æç³»ç»Ÿæ€§èƒ½"""
    print("\n" + "="*60)
    print("ğŸ“Š ç³»ç»Ÿæ€§èƒ½åˆ†æ")
    print("="*60)
    
    # è·å–å¤æ‚åº¦ç»Ÿè®¡
    if hasattr(rag.router, 'get_complexity_stats'):
        stats = rag.router.get_complexity_stats()
        print(f"å¤æ‚åº¦åˆ†ç±»ç»Ÿè®¡: {stats}")
    
    # æ£€æŸ¥å­˜å‚¨çŠ¶æ€
    working_dir = rag.working_dir
    print(f"\nå­˜å‚¨çŠ¶æ€:")
    print(f"å·¥ä½œç›®å½•: {working_dir}")
    
    # æ£€æŸ¥å„ç§æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    files_to_check = [
        "vdb_chunks.json",
        "vdb_entities.json", 
        "kv_store_text_chunks.json",
        "kv_store_entities.json",
        "kv_store_community_reports.json",
        "graph_chunk_entity_relation.graphml"
    ]
    
    for file_name in files_to_check:
        file_path = os.path.join(working_dir, file_name)
        exists = os.path.exists(file_path)
        size = os.path.getsize(file_path) if exists else 0
        print(f"  {file_name}: {'âœ…' if exists else 'âŒ'} ({size} bytes)")


#------------------------------------------------------------------------------
# ä¸»å‡½æ•°
#------------------------------------------------------------------------------

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¤æ‚åº¦åˆ†ç±»å™¨å®Œæ•´RAGæµç¨‹æ¼”ç¤º")
    print("="*60)
    
    # æ£€æŸ¥APIå¯†é’¥
    if DASHSCOPE_API_KEY == "YOUR_DASHSCOPE_API_KEY":
        print("âš ï¸ è¯·è®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡")
        print("ä¾‹å¦‚: export DASHSCOPE_API_KEY=your_api_key_here")
        print("æˆ–è€…åœ¨ä»£ç ä¸­ç›´æ¥è®¾ç½®DASHSCOPE_API_KEYå˜é‡")
        return
    
    # 1. å‡†å¤‡æ–‡æ¡£
    print("\nğŸ“ æ­¥éª¤1: å‡†å¤‡æ–‡æ¡£æ•°æ®")
    documents = prepare_documents()
    print(f"å‡†å¤‡äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    # ä¿å­˜æ–‡æ¡£åˆ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    docs_file = os.path.join(WORKING_DIR, "sample_documents.txt")
    os.makedirs(WORKING_DIR, exist_ok=True)
    save_documents_to_file(documents, docs_file)
    print(f"æ–‡æ¡£å·²ä¿å­˜åˆ°: {docs_file}")
    
    # 2. æµ‹è¯•å¤æ‚åº¦åˆ†ç±»å™¨
    print("\nğŸ§  æ­¥éª¤2: æµ‹è¯•å¤æ‚åº¦åˆ†ç±»å™¨")
    router = create_complexity_router()
    await test_complexity_classification(router)
    
    # 3. æ„å»ºRAGç³»ç»Ÿ
    print("\nğŸ”§ æ­¥éª¤3: æ„å»ºRAGç³»ç»Ÿ")
    rag = build_rag_system(WORKING_DIR)
    
    # 4. æ’å…¥æ–‡æ¡£
    print("\nğŸ“š æ­¥éª¤4: æ’å…¥æ–‡æ¡£")
    await insert_documents(rag, documents)
    
    # 5. æµ‹è¯•å¤æ‚åº¦æ„ŸçŸ¥æŸ¥è¯¢
    print("\nğŸ” æ­¥éª¤5: æµ‹è¯•å¤æ‚åº¦æ„ŸçŸ¥æŸ¥è¯¢")
    await test_queries_with_complexity_routing(rag)
    
    # 6. æµ‹è¯•ä¸åŒæ¨¡å¼
    print("\nğŸ”„ æ­¥éª¤6: æµ‹è¯•ä¸åŒæ£€ç´¢æ¨¡å¼")
    test_different_modes(rag)
    
    # 7. æ€§èƒ½åˆ†æ
    print("\nğŸ“Š æ­¥éª¤7: ç³»ç»Ÿæ€§èƒ½åˆ†æ")
    analyze_system_performance(rag)
    
    print("\n" + "="*60)
    print("ğŸ‰ å¤æ‚åº¦åˆ†ç±»å™¨å®Œæ•´RAGæµç¨‹æ¼”ç¤ºå®Œæˆï¼")
    print("="*60)
    
    print("\nğŸ“ ä½¿ç”¨è¯´æ˜:")
    print("1. ç³»ç»Ÿå·²è‡ªåŠ¨æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦é€‰æ‹©æœ€ä½³æ£€ç´¢æ¨¡å¼")
    print("2. zero_hopæŸ¥è¯¢ä½¿ç”¨llm_onlyæ¨¡å¼ï¼ˆç›´æ¥å›ç­”ï¼‰")
    print("3. one_hopæŸ¥è¯¢ä½¿ç”¨naiveæˆ–bm25æ¨¡å¼ï¼ˆå•æ­¥æ£€ç´¢ï¼‰")
    print("4. multi_hopæŸ¥è¯¢ä½¿ç”¨localæˆ–globalæ¨¡å¼ï¼ˆå›¾æ¨ç†ï¼‰")
    print("5. å¦‚æœæ¨¡å‹ç½®ä¿¡åº¦ä½ï¼Œä¼šè‡ªåŠ¨å›é€€åˆ°è§„åˆ™åˆ†ç±»")


if __name__ == "__main__":
    # è¿è¡Œä¸»å‡½æ•°
    asyncio.run(main()) 